{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:31.132750700Z",
     "start_time": "2023-09-09T19:20:31.117340600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a5b2fc17a89d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some insights into this project:\n",
    "- The category of machine learning models that these fall into is _supervised learning_. Supervised learning is a type of machine learning where the model is trained on labeled data. The label in this case is the method of data breach.\n",
    "\n",
    "The models that will be tested out will fall into the following category of machine learning:\n",
    "* <u><b>Logistic Regression</b></u>: This is a supervised Learning that can be used to predict a categorical outcome. In this case, the categorical outcome is the method of the data breach. The features that will be used are: entity, year, records and organization type.\n",
    "* <u><b>Decision Trees</b></u>: This is another supervised learning algorithm that can be used to predict a categorical outcome. Decision trees work by creating a tree-like structure that represents the relationships between the features and the outcomes.\n",
    "* <u><b>Support Vector Machine(SVMs):</u></b> This is a supervised learning algorithm that can be used to predict both categorical and continous outcomes. SVMs work by finding the hyperplane that best seperates the data points into different classes.\n",
    "* <u><b>Random Forests</u></b> This is an ensemble learning algorithm that combines multiple decision trees to improve the accuracy of the predictions.\n",
    "* <u><b>Neural Networks</u></b> This is a more complex algorithm that can be used to predict both categorical and continuous outcomes. Neural Networks work by learning the relationships between the features and the outcome through a process called backpropogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d4a0b8ccba56a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:31.530128100Z",
     "start_time": "2023-09-09T19:20:31.452203700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Records</th>\n",
       "      <th>Organization type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21st Century Oncology</td>\n",
       "      <td>2016</td>\n",
       "      <td>2200000</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[5][6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>500px</td>\n",
       "      <td>2020</td>\n",
       "      <td>14870304</td>\n",
       "      <td>social networking</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Accendo Insurance Co.</td>\n",
       "      <td>2020</td>\n",
       "      <td>175350</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[8][9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adobe Systems Incorporated</td>\n",
       "      <td>2013</td>\n",
       "      <td>152000000</td>\n",
       "      <td>tech</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adobe Inc.</td>\n",
       "      <td>2019</td>\n",
       "      <td>7500000</td>\n",
       "      <td>tech</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[11][12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Advocate Medical Group</td>\n",
       "      <td>2017</td>\n",
       "      <td>4000000</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>lost / stolen media</td>\n",
       "      <td>[13][14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>AerServ (subsidiary of InMobi)</td>\n",
       "      <td>2018</td>\n",
       "      <td>75000</td>\n",
       "      <td>advertising</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Affinity Health Plan, Inc.</td>\n",
       "      <td>2013</td>\n",
       "      <td>344579</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>lost / stolen media</td>\n",
       "      <td>[16][17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Airtel</td>\n",
       "      <td>2019</td>\n",
       "      <td>320000000</td>\n",
       "      <td>telecommunications</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Air Canada</td>\n",
       "      <td>2018</td>\n",
       "      <td>20000</td>\n",
       "      <td>transport</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Entity  Year    Records  \\\n",
       "0           0           21st Century Oncology  2016    2200000   \n",
       "1           1                           500px  2020   14870304   \n",
       "2           2           Accendo Insurance Co.  2020     175350   \n",
       "3           3      Adobe Systems Incorporated  2013  152000000   \n",
       "4           4                      Adobe Inc.  2019    7500000   \n",
       "5           5          Advocate Medical Group  2017    4000000   \n",
       "6           6  AerServ (subsidiary of InMobi)  2018      75000   \n",
       "7           7      Affinity Health Plan, Inc.  2013     344579   \n",
       "8           8                          Airtel  2019  320000000   \n",
       "9           9                      Air Canada  2018      20000   \n",
       "\n",
       "    Organization type               Method   Sources  \n",
       "0          healthcare               hacked    [5][6]  \n",
       "1   social networking               hacked       [7]  \n",
       "2          healthcare        poor security    [8][9]  \n",
       "3                tech               hacked      [10]  \n",
       "4                tech        poor security  [11][12]  \n",
       "5          healthcare  lost / stolen media  [13][14]  \n",
       "6         advertising               hacked      [15]  \n",
       "7          healthcare  lost / stolen media  [16][17]  \n",
       "8  telecommunications        poor security      [18]  \n",
       "9           transport               hacked      [19]  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset(s) we will be using\n",
    "csv_file_path = os.path.abspath('df_1.csv');  # get the absolute path of the CS\n",
    "\n",
    "df = pd.read_csv(csv_file_path);   # Read the CSV file into a datafram\n",
    "# display the head to see if the dataset works as intended\n",
    "df.head(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3f1621f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            int64\n",
       "Entity               object\n",
       "Year                 object\n",
       "Records              object\n",
       "Organization type    object\n",
       "Method               object\n",
       "Sources              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes  # simply lists out the datatype we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f4b9e109abfaf428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:33.325609Z",
     "start_time": "2023-09-09T19:20:33.265630700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Entity', 'Year', 'Records', 'Organization type',\n",
       "       'Method', 'Sources'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # observe the column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c0cc115ab662e2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:33.816103200Z",
     "start_time": "2023-09-09T19:20:33.785937900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will need to rename the unnamed column\n",
    "df.rename(columns={'Unnamed: 0' : 'Index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b20893287fb448ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:35.163638500Z",
     "start_time": "2023-09-09T19:20:35.143525800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the number of unique values contained in Entity\n",
    "len(df['Entity'].unique())   #there's a total of 331 unique name for comapnies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e84958bb201b54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:37.803496900Z",
     "start_time": "2023-09-09T19:20:37.688758800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1 = df.copy()  # we don't want to make modifications to the original dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8603b602fd5debd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.813591Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Records</th>\n",
       "      <th>Organization type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21st Century Oncology</td>\n",
       "      <td>2016</td>\n",
       "      <td>2200000</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[5][6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>500px</td>\n",
       "      <td>2020</td>\n",
       "      <td>14870304</td>\n",
       "      <td>social networking</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Accendo Insurance Co.</td>\n",
       "      <td>2020</td>\n",
       "      <td>175350</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[8][9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adobe Systems Incorporated</td>\n",
       "      <td>2013</td>\n",
       "      <td>152000000</td>\n",
       "      <td>tech</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adobe Inc.</td>\n",
       "      <td>2019</td>\n",
       "      <td>7500000</td>\n",
       "      <td>tech</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[11][12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>347</td>\n",
       "      <td>Zynga</td>\n",
       "      <td>2019</td>\n",
       "      <td>173000000</td>\n",
       "      <td>social network</td>\n",
       "      <td>hacked</td>\n",
       "      <td>[406][407]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>348</td>\n",
       "      <td>Unknown agency(believed to be tied to United S...</td>\n",
       "      <td>2020</td>\n",
       "      <td>200000000</td>\n",
       "      <td>financial</td>\n",
       "      <td>accidentally published</td>\n",
       "      <td>[408]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>National Health Information Center (NCZI) of S...</td>\n",
       "      <td>2020</td>\n",
       "      <td>391250</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[409]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>350</td>\n",
       "      <td>50 companies and government institutions</td>\n",
       "      <td>2022</td>\n",
       "      <td>6400000</td>\n",
       "      <td>various</td>\n",
       "      <td>poor security</td>\n",
       "      <td>[410] [411]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>351</td>\n",
       "      <td>IKEA</td>\n",
       "      <td>2022</td>\n",
       "      <td>95000</td>\n",
       "      <td>retail</td>\n",
       "      <td>accidentally published</td>\n",
       "      <td>[412]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index                                             Entity  Year  \\\n",
       "0        0                              21st Century Oncology  2016   \n",
       "1        1                                              500px  2020   \n",
       "2        2                              Accendo Insurance Co.  2020   \n",
       "3        3                         Adobe Systems Incorporated  2013   \n",
       "4        4                                         Adobe Inc.  2019   \n",
       "..     ...                                                ...   ...   \n",
       "347    347                                              Zynga  2019   \n",
       "348    348  Unknown agency(believed to be tied to United S...  2020   \n",
       "349    349  National Health Information Center (NCZI) of S...  2020   \n",
       "350    350           50 companies and government institutions  2022   \n",
       "351    351                                               IKEA  2022   \n",
       "\n",
       "       Records  Organization type                  Method      Sources  \n",
       "0      2200000         healthcare                  hacked       [5][6]  \n",
       "1     14870304  social networking                  hacked          [7]  \n",
       "2       175350         healthcare           poor security       [8][9]  \n",
       "3    152000000               tech                  hacked         [10]  \n",
       "4      7500000               tech           poor security     [11][12]  \n",
       "..         ...                ...                     ...          ...  \n",
       "347  173000000     social network                  hacked   [406][407]  \n",
       "348  200000000          financial  accidentally published        [408]  \n",
       "349     391250         healthcare           poor security        [409]  \n",
       "350    6400000            various           poor security  [410] [411]  \n",
       "351      95000             retail  accidentally published        [412]  \n",
       "\n",
       "[352 rows x 7 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_copy1['Organization type'].unique())   # in terms of organization type, there's 70 different unique values\n",
    "df_copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38dc8a351025f36a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.833091200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the special characters\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace(',', '_')  # replace the cases of ',' with '_'.\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace(' ', '_') # we also replace the cases of spacing with '_'.\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace('__', '_')  # replace all instances of __ with _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdeb53",
   "metadata": {},
   "source": [
    "Before encoding the organization type, I've copied the df to keep the original data intact. I'm dropping the hashed_Entity column as it is not needed for visualizing – we want to focus on industries and not individual companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1ce8d0263117907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.847272400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Records</th>\n",
       "      <th>Organization type</th>\n",
       "      <th>Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21st Century Oncology</td>\n",
       "      <td>2016</td>\n",
       "      <td>2200000</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>hacked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500px</td>\n",
       "      <td>2020</td>\n",
       "      <td>14870304</td>\n",
       "      <td>social networking</td>\n",
       "      <td>hacked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accendo Insurance Co.</td>\n",
       "      <td>2020</td>\n",
       "      <td>175350</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>poor security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adobe Systems Incorporated</td>\n",
       "      <td>2013</td>\n",
       "      <td>152000000</td>\n",
       "      <td>tech</td>\n",
       "      <td>hacked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adobe Inc.</td>\n",
       "      <td>2019</td>\n",
       "      <td>7500000</td>\n",
       "      <td>tech</td>\n",
       "      <td>poor security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Zynga</td>\n",
       "      <td>2019</td>\n",
       "      <td>173000000</td>\n",
       "      <td>social network</td>\n",
       "      <td>hacked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Unknown agency(believed to be tied to United S...</td>\n",
       "      <td>2020</td>\n",
       "      <td>200000000</td>\n",
       "      <td>financial</td>\n",
       "      <td>accidentally published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>National Health Information Center (NCZI) of S...</td>\n",
       "      <td>2020</td>\n",
       "      <td>391250</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>poor security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>50 companies and government institutions</td>\n",
       "      <td>2022</td>\n",
       "      <td>6400000</td>\n",
       "      <td>various</td>\n",
       "      <td>poor security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>IKEA</td>\n",
       "      <td>2022</td>\n",
       "      <td>95000</td>\n",
       "      <td>retail</td>\n",
       "      <td>accidentally published</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Entity  Year    Records  \\\n",
       "0                                21st Century Oncology  2016    2200000   \n",
       "1                                                500px  2020   14870304   \n",
       "2                                Accendo Insurance Co.  2020     175350   \n",
       "3                           Adobe Systems Incorporated  2013  152000000   \n",
       "4                                           Adobe Inc.  2019    7500000   \n",
       "..                                                 ...   ...        ...   \n",
       "347                                              Zynga  2019  173000000   \n",
       "348  Unknown agency(believed to be tied to United S...  2020  200000000   \n",
       "349  National Health Information Center (NCZI) of S...  2020     391250   \n",
       "350           50 companies and government institutions  2022    6400000   \n",
       "351                                               IKEA  2022      95000   \n",
       "\n",
       "     Organization type                  Method  \n",
       "0           healthcare                  hacked  \n",
       "1    social networking                  hacked  \n",
       "2           healthcare           poor security  \n",
       "3                 tech                  hacked  \n",
       "4                 tech           poor security  \n",
       "..                 ...                     ...  \n",
       "347     social network                  hacked  \n",
       "348          financial  accidentally published  \n",
       "349         healthcare           poor security  \n",
       "350            various           poor security  \n",
       "351             retail  accidentally published  \n",
       "\n",
       "[352 rows x 5 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis = df.copy()  #create another copy of the df_copy and perform further data preprocessing\n",
    "#drop the Sources column\n",
    "df_vis.drop(columns={'Sources'}, inplace=True)\n",
    "df_vis.drop(columns={'Index'}, inplace=True)\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-integer values in the 'Records' column:\n",
      "Index, Value, Industry\n",
      "10 unknown web\n",
      "25 unknown dating\n",
      "27 G20 world leaders government\n",
      "28 19 years of data academic\n",
      "34 63 stores retail\n",
      "40 nan gaming\n",
      "41 unknown gambling\n",
      "47 tens of thousands retail\n",
      "48 over 5,000,000 government\n",
      "66 unknown (client list) information technology\n",
      "76 millions retail\n",
      "80 235 GB military\n",
      "82 350 clients emails consulting, accounting\n",
      "94 9,000,000 (approx) - basic booking, 2208 (credit card details) transport\n",
      "104 unknown financial\n",
      "109 unknown tech\n",
      "119 Unknown Information Security\n",
      "134 unknown web\n",
      "140 unknown healthcare\n",
      "142 2.5GB transport\n",
      "147 250 locations hotel\n",
      "163 unknown mobile carrier\n",
      "168 500 locations restaurant\n",
      "175 10 locations hotel\n",
      "185 unknown software\n",
      "198 unknown financial\n",
      "199 93 stores retail\n",
      "215 unknown telecom\n",
      "220 unknown government\n",
      "221 undisclosed web\n",
      "232 unknown web\n",
      "235 unknown hotel\n",
      "243 unknown tech\n",
      "248 Source Code Compromised Network Monitoring\n",
      "251 100 terabytes media\n",
      "260 54 locations hotel\n",
      "265 200 stores retail\n",
      "286 8 locations hotel\n",
      "288 unknown tech\n",
      "289 unknown tech\n",
      "291 unknown tech\n",
      "294 unknown gaming\n",
      "310 51 locations retail\n",
      "315 TBC government, military\n",
      "334 unknown restaurant\n",
      "336 unknown arts group\n",
      "337 nan web service\n",
      "Unique industries from non-integer records:\n",
      "['tech', 'gaming', 'consulting, accounting', 'retail', 'web', 'Network Monitoring', 'restaurant', 'information technology', 'software', 'dating', 'gambling', 'transport', 'military', 'government, military', 'financial', 'hotel', 'mobile carrier', 'media', 'Information Security', 'healthcare', 'government', 'arts group', 'web service', 'academic', 'telecom']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store index positions and non-integer values as tuples\n",
    "non_integer_records = []\n",
    "\n",
    "# Initialize a set to store unique industries\n",
    "unique_industries = set()\n",
    "\n",
    "# Iterate through the 'Records' column and collect non-integer values with their index positions and industry\n",
    "for index, (value, industry) in enumerate(zip(df_vis['Records'], df_vis['Organization type'])):\n",
    "    try:\n",
    "        int_value = int(value)\n",
    "    except ValueError:\n",
    "        non_integer_records.append((index, str(value), industry))\n",
    "        unique_industries.add(industry)\n",
    "\n",
    "# Print the index positions, values, and industries for non-integer values\n",
    "if non_integer_records:\n",
    "    print(\"Non-integer values in the 'Records' column:\")\n",
    "    print(\"Index, Value, Industry\")\n",
    "    for index, value, industry in non_integer_records:\n",
    "        print(index, value, industry)\n",
    "else:\n",
    "    print(\"No non-integer values in the 'Records' column.\")\n",
    "    \n",
    "# Print the list of unique industries from non-integer records\n",
    "if unique_industries:\n",
    "    print(\"Unique industries from non-integer records:\")\n",
    "    print(list(unique_industries))\n",
    "else:\n",
    "    print(\"No unique industries found in non-integer records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated non-integer records: ['consulting, accounting', 'Network Monitoring', 'information technology', 'software', 'dating', 'gambling', 'Information Security', 'arts group', 'web service']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/r_8fkf9s2ys9d8p1lfwj100r0000gn/T/ipykernel_80505/819895320.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_df['Records'] = pd.to_numeric(industry_df['Records'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "updated_non_integer_records = []\n",
    "# types = list()\n",
    "\n",
    "for industry in unique_industries:\n",
    "    # Create a separate DataFrame for the current industry\n",
    "    industry_df = df_vis[df_vis['Organization type'] == industry]\n",
    "    \n",
    "    # Drop entries with non-integer values in 'Records'\n",
    "    industry_df['Records'] = pd.to_numeric(industry_df['Records'], errors='coerce')\n",
    "    industry_df = industry_df.dropna()\n",
    "    \n",
    "    # Calculate the mean records for the current industry\n",
    "    mean_records = industry_df['Records'].mean()\n",
    "    \n",
    "    # #add the type of mean_records to the types list\n",
    "    # types.append(type(mean_records))\n",
    "    \n",
    "    # Update 'Records' in the original DataFrame if mean is not NaN and remove from non_integer_records\n",
    "    #if the mean_record is nan, its type will be float\n",
    "    if type(mean_records) != float:\n",
    "        # Print the mean records for the current industry\n",
    "        # print(f\"Industry: {industry}, Type: {type(mean_records)}, Mean Records: {mean_records}\")\n",
    "        for index, value, industry_name in non_integer_records:\n",
    "            if industry_name == industry:\n",
    "                df_vis.at[index, 'Records'] = mean_records\n",
    "    else:\n",
    "        updated_non_integer_records.append(industry)\n",
    "\n",
    "#make the types list a set\n",
    "# types = set(types)\n",
    "# print(f\"Types of mean_records: {types}\")\n",
    "print(f\"Updated non-integer records: {updated_non_integer_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No remaining non-integer values in the 'Records' column!\n",
      "Entity               object\n",
      "Year                 object\n",
      "Records               int64\n",
      "Organization type    object\n",
      "Method               object\n",
      "dtype: object\n",
      "                                                Entity  Year    Records  \\\n",
      "0                                21st Century Oncology  2016    2200000   \n",
      "1                                                500px  2020   14870304   \n",
      "2                                Accendo Insurance Co.  2020     175350   \n",
      "3                           Adobe Systems Incorporated  2013  152000000   \n",
      "4                                           Adobe Inc.  2019    7500000   \n",
      "..                                                 ...   ...        ...   \n",
      "347                                              Zynga  2019  173000000   \n",
      "348  Unknown agency(believed to be tied to United S...  2020  200000000   \n",
      "349  National Health Information Center (NCZI) of S...  2020     391250   \n",
      "350           50 companies and government institutions  2022    6400000   \n",
      "351                                               IKEA  2022      95000   \n",
      "\n",
      "     Organization type                  Method  \n",
      "0           healthcare                  hacked  \n",
      "1    social networking                  hacked  \n",
      "2           healthcare           poor security  \n",
      "3                 tech                  hacked  \n",
      "4                 tech           poor security  \n",
      "..                 ...                     ...  \n",
      "347     social network                  hacked  \n",
      "348          financial  accidentally published  \n",
      "349         healthcare           poor security  \n",
      "350            various           poor security  \n",
      "351             retail  accidentally published  \n",
      "\n",
      "[342 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean records value for the entire DataFrame df_vis (excluding non-numeric values)\n",
    "df_vis['Records'] = pd.to_numeric(df_vis['Records'], errors='coerce')\n",
    "df_vis = df_vis.dropna()\n",
    "mean_records_all = df_vis['Records'].mean()\n",
    "\n",
    "# Replace 'Records' values in updated_non_integer_records with the mean records for the entire DataFrame\n",
    "for industry in updated_non_integer_records:\n",
    "    # for each entry in the dataframe, if the industry matches the industry in the updated_non_integer_records, replace the value with the mean_records_all\n",
    "    for index, value, industry_name in non_integer_records:\n",
    "        if industry_name == industry:\n",
    "            df_vis.at[index, 'Records'] = mean_records_all\n",
    "#print(df_vis)\n",
    "#Check that there are no more non-integer values in 'Records'\n",
    "non_integer_records = []\n",
    "for index, value, industry in zip(df_vis.index, df_vis['Records'], df_vis['Organization type']):\n",
    "    try:\n",
    "        int_value = int(value)\n",
    "    except ValueError:\n",
    "        non_integer_records.append((index, str(value), industry))\n",
    "\n",
    "if non_integer_records:\n",
    "    print(\"Non-integer values in the 'Records' column:\")\n",
    "    print(\"Index, Value, Industry\")\n",
    "    for index, value, industry in non_integer_records:\n",
    "        print(index, value, industry)\n",
    "else:\n",
    "    print(\"No remaining non-integer values in the 'Records' column!\")\n",
    "\n",
    "#Update all the values in the Records column to be integers\n",
    "df_vis['Records'] = df_vis['Records'].astype(int)\n",
    "\n",
    "# Check the data types of the DataFrame in the 'Records' column\n",
    "print(df_vis.dtypes)\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633714",
   "metadata": {},
   "source": [
    "Checking if the Year column is well formatted (should be a year like 2016, or 2019, not 2016-2019). If not, we'll need to do some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "51564148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values:  False\n",
      "Non numeric values:  True\n",
      "                                                Entity           Year  \\\n",
      "94                                             EasyJet      2019-2020   \n",
      "96   Earl Enterprises(Buca di Beppo, Earl of Sandwi...      2018-2019   \n",
      "144                                      Hilton Hotels  2014 and 2015   \n",
      "\n",
      "      Records Organization type  Method  \n",
      "94   13394400         transport  hacked  \n",
      "96    2000000        restaurant  hacked  \n",
      "144    363000             hotel  hacked  \n"
     ]
    }
   ],
   "source": [
    "#Check if any value in the year column is null\n",
    "print(\"Null values: \", df_vis['Year'].isnull().values.any())  # there are no null values in the year column\n",
    "\n",
    "#Check if any value in the year column is not well formatted (i.e. not a number)\n",
    "print(\"Non numeric values: \", df_vis['Year'].str.isnumeric().values.any())  # there are values that are not numeric\n",
    "\n",
    "#print out all the values in the year column that are not numeric\n",
    "print(df_vis[~df_vis['Year'].str.isnumeric()])  # as we can see, the three columns that have non-numeric values are in index 94 96 and 144\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40deac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Entity  Year    Records  \\\n",
      "0                                21st Century Oncology  2016    2200000   \n",
      "1                                                500px  2020   14870304   \n",
      "2                                Accendo Insurance Co.  2020     175350   \n",
      "3                           Adobe Systems Incorporated  2013  152000000   \n",
      "4                                           Adobe Inc.  2019    7500000   \n",
      "..                                                 ...   ...        ...   \n",
      "347                                              Zynga  2019  173000000   \n",
      "348  Unknown agency(believed to be tied to United S...  2020  200000000   \n",
      "349  National Health Information Center (NCZI) of S...  2020     391250   \n",
      "350           50 companies and government institutions  2022    6400000   \n",
      "351                                               IKEA  2022      95000   \n",
      "\n",
      "     Organization type                  Method  \n",
      "0           healthcare                  hacked  \n",
      "1    social networking                  hacked  \n",
      "2           healthcare           poor security  \n",
      "3                 tech                  hacked  \n",
      "4                 tech           poor security  \n",
      "..                 ...                     ...  \n",
      "347     social network                  hacked  \n",
      "348          financial  accidentally published  \n",
      "349         healthcare           poor security  \n",
      "350            various           poor security  \n",
      "351             retail  accidentally published  \n",
      "\n",
      "[342 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_vis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c521a8b",
   "metadata": {},
   "source": [
    "There are 3 values in the Year column that are not well formed – we also will need to fix the Records column for similar formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6a650533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity               object\n",
      "Year                  int64\n",
      "Records               int64\n",
      "Organization type    object\n",
      "Method               object\n",
      "dtype: object\n",
      "                                                Entity  Year    Records  \\\n",
      "0                                21st Century Oncology  2016    2200000   \n",
      "1                                                500px  2020   14870304   \n",
      "2                                Accendo Insurance Co.  2020     175350   \n",
      "3                           Adobe Systems Incorporated  2013  152000000   \n",
      "4                                           Adobe Inc.  2019    7500000   \n",
      "..                                                 ...   ...        ...   \n",
      "340           50 companies and government institutions  2022    6400000   \n",
      "341                                               IKEA  2022      95000   \n",
      "342                                            EasyJet  2020    6697200   \n",
      "343  Earl Enterprises(Buca di Beppo, Earl of Sandwi...  2019    1000000   \n",
      "344                                      Hilton Hotels  2015     181500   \n",
      "\n",
      "     Organization type                  Method  \n",
      "0           healthcare                  hacked  \n",
      "1    social networking                  hacked  \n",
      "2           healthcare           poor security  \n",
      "3                 tech                  hacked  \n",
      "4                 tech           poor security  \n",
      "..                 ...                     ...  \n",
      "340            various           poor security  \n",
      "341             retail  accidentally published  \n",
      "342          transport                  hacked  \n",
      "343         restaurant                  hacked  \n",
      "344              hotel                  hacked  \n",
      "\n",
      "[345 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Modify df_vis:\n",
    "for each value in the year column that is not numeric:\n",
    "    record the last year listed in the year column (i.e. the last 4 characters)\n",
    "    record the first year listed in the year column (i.e. the first 4 characters)\n",
    "change the year column of this entry to the first 4 characters (the first year)\n",
    "for each year between the first year and the last year:\n",
    "    add a new entry to the dataframe with the same values as the entry that was changed, except for the year column, which will be the year in question\n",
    "'''\n",
    "last_row_index = df_vis.tail(1).index[0]\n",
    "# Create an empty list to store modified rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df_vis.iterrows():\n",
    "    year_value = row['Year']\n",
    "    \n",
    "    # Check if the year is not numeric\n",
    "    if not year_value.isnumeric():\n",
    "        # Extract the first and last year\n",
    "        first_year = int(year_value[:4])\n",
    "        last_year = int(year_value[-4:])\n",
    "        \n",
    "        # Calculate the range of years and ensure it's at least 1\n",
    "        year_range = max(last_year - first_year, 0) + 1\n",
    "\n",
    "        # Change the year column to the first year\n",
    "        df_vis.loc[index, 'Year'] = str(first_year)\n",
    "        \n",
    "        # Calculate the records divided by the number of years in the range\n",
    "        records_divided = row['Records'] / year_range\n",
    "        \n",
    "        # Update the records column with the new value\n",
    "        df_vis.loc[index, 'Records'] = records_divided\n",
    "        \n",
    "        #Create new rows for each year between the first and last year\n",
    "        for year in range(first_year + 1, last_year + 1):\n",
    "            new_row = row.copy()  # Create a copy of the current row\n",
    "            new_row['Year'] = str(year)\n",
    "            new_row['Records'] = records_divided\n",
    "            new_rows.append(new_row) # Append the new row to the list\n",
    "\n",
    "# Concatenate the new rows with the original DataFrame\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "df_vis = pd.concat([df_vis, new_rows_df], ignore_index=True)\n",
    "\n",
    "# Convert Records to int\n",
    "df_vis['Records'] = df_vis['Records'].astype(int)\n",
    "\n",
    "# convert the year column to int\n",
    "df_vis['Year'] = df_vis['Year'].astype(int)\n",
    "\n",
    "\n",
    "print(df_vis.dtypes)\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21st_Century_Oncology' '500px' 'Accendo_Insurance_Co.'\n",
      " 'Adobe_Systems_Incorporated' 'Adobe_Inc.' 'Advocate_Medical_Group'\n",
      " 'AerServ_(subsidiary_of_InMobi)' 'Affinity_Health_Plan_Inc.' 'Airtel'\n",
      " 'Air_Canada' 'Amazon_Japan_G.K.' 'TD_Ameritrade' 'Ancestry.com'\n",
      " 'Animal_Jam' 'Ankle_&_Foot_Center_of_Tampa_Bay_Inc.' 'Anthem_Inc.' 'AOL'\n",
      " 'Apple_Inc._BlueToad' 'Apple' 'Apple_Health_Medicaid' 'Ashley_Madison'\n",
      " 'AT&T' 'Auction.co.kr' 'Australian_Immigration_Department'\n",
      " 'Australian_National_University' 'Automatic_Data_Processing' 'AvMed_Inc.'\n",
      " \"Bailey's_Inc.\" 'The_Bank_of_New_York_Mellon' 'Bank_of_America'\n",
      " 'Barnes_&_Noble' 'Bell_Canada' 'Benesse' 'Betfair'\n",
      " 'Bethesda_Game_Studios' 'Blank_Media_Games' 'Blizzard_Entertainment'\n",
      " 'BlueCross_BlueShield_of_Tennessee' 'BMO_and_Simplii'\n",
      " '2018_British_Airways_cyberattack' 'British_Airways'\n",
      " '2019_Bulgarian_revenue_agency_hack'\n",
      " 'California_Department_of_Child_Support_Services' 'Canva' 'Capcom'\n",
      " 'Capital_One'\n",
      " 'CardSystems_Solutions_Inc._(MasterCard_Visa_Discover_Financial_Services_and_American_Express)'\n",
      " 'Cathay_Pacific_Airways' 'CareFirst_BlueCross_Blue_Shield__Maryland'\n",
      " 'Central_Coast_Credit_Union' 'Central_Hudson_Gas_&_Electric'\n",
      " 'CheckFree_Corporation' 'CheckPeople' 'China_Software_Developer_Network'\n",
      " 'Chinese_gaming_websites_(three:_Duowan_7K7K_178.com)' 'Citigroup'\n",
      " 'City_and_Hackney_Teaching_Primary_Care_Trust' 'Colorado_government'\n",
      " 'Community_Health_Systems' 'Philippines_Commission_on_Elections'\n",
      " 'Compass_Bank' 'Countrywide_Financial_Corp'\n",
      " 'Centers_for_Medicare_&_Medicaid_Services' 'Cox_Communications'\n",
      " 'Crescent_Health_Inc._Walgreens' 'CVS' 'CyberServe' 'Dai_Nippon_Printing'\n",
      " 'Data_Processors_International_(MasterCard_Visa_Discover_Financial_Services_and_American_Express)'\n",
      " 'Defense_Integrated_Data_Center_(South_Korea)' 'Dedalus'\n",
      " 'US_Department_of_Homeland_Security' 'Desjardins'\n",
      " \"Domino's_Pizza_(France)\" 'DoorDash' 'UK_Driving_Standards_Agency'\n",
      " 'Dropbox' 'Drupal' 'DSW_Inc.' 'Dubsmash' 'Dun_&_Bradstreet' 'EasyJet'\n",
      " 'eBay'\n",
      " 'Earl_Enterprises(Buca_di_Beppo_Earl_of_Sandwich_Planet_Hollywood_Chicken_Guy_Mixology_Tequila_Taqueria)'\n",
      " 'Educational_Credit_Management_Corporation' 'Eisenhower_Medical_Center'\n",
      " 'ElasticSearch' 'Embassy_Cables' 'Emergency_Healthcare_Physicians_Ltd.'\n",
      " 'Emory_Healthcare' 'Equifax' 'European_Central_Bank' 'Evernote' 'Exactis'\n",
      " 'Excellus_BlueCross_BlueShield' 'Experian__T_Mobile_US' 'EyeWire'\n",
      " 'Facebook' 'Fast_Retailing' 'Federal_Reserve_Bank_of_Cleveland'\n",
      " 'Fidelity_National_Information_Services' 'First_American_Corporation'\n",
      " 'Florida_Department_of_Juvenile_Justice' 'Friend_Finder_Networks'\n",
      " 'Funimation' 'Formspring' 'Unknown' 'Gamigo' 'Gap_Inc.' 'Gawker'\n",
      " 'Global_Payments' 'Gmail' 'Google_Plus' 'Greek_government'\n",
      " 'Grozio_Chirurgija' 'GS_Caltex' 'Gyft'\n",
      " 'Hannaford_Brothers_Supermarket_Chain' 'HauteLook' 'Health_Net'\n",
      " 'Health_Net\\xa0—_IBM' 'Health_Sciences_Authority_(Singapore)'\n",
      " 'Health_Service_Executive' 'Heartland' 'Heathrow_Airport'\n",
      " 'Hewlett_Packard' 'Hilton_Hotels' 'Home_Depot' 'Honda_Canada'\n",
      " 'Hyatt_Hotels' 'Iberdrola' 'Instagram' 'Internal_Revenue_Service'\n",
      " 'International_Committee_of_the_Red_Cross' 'Inuvik_hospital'\n",
      " 'Iranian_banks_(three:_Saderat_Eghtesad_Novin_and_Saman)'\n",
      " 'Japan_Pension_Service' 'Japanet_Takata' 'Jefferson_County_West_Virginia'\n",
      " 'JP_Morgan_Chase' 'Justdial' 'KDDI' 'Kirkwood_Community_College' 'KM.RU'\n",
      " 'Koodo_Mobile' 'Korea_Credit_Bureau' 'Kroll_Background_America'\n",
      " 'KT_Corporation' 'LexisNexis' \"Landry's_Inc.\"\n",
      " 'Les_Éditions_Protégez_vous' 'LifeLabs'\n",
      " 'Lincoln_Medical_&_Mental_Health_Center' 'LinkedIn_eHarmony_Last.fm'\n",
      " 'Living_Social' 'MacRumors.com' 'Mandarin_Oriental_Hotels'\n",
      " 'Marriott_International' 'Massachusetts_Government'\n",
      " 'Massive_American_business_hack_including_7_Eleven_and_Nasdaq'\n",
      " 'US_Medicaid' 'Medical_Informatics_Engineering'\n",
      " 'Memorial_Healthcare_System' 'Michaels' 'Microsoft' 'Militarysingles.com'\n",
      " 'Ministry_of_Education_(Chile)' 'Ministry_of_Health_(Singapore)'\n",
      " 'Mitsubishi_Tokyo_UFJ_Bank' 'MongoDB' 'Mobile_TeleSystems_(MTS)'\n",
      " 'Monster.com' 'Morgan_Stanley_Smith_Barney' 'Morinaga_Confectionery'\n",
      " 'Mozilla' 'MyHeritage' 'NASDAQ' 'Natural_Grocers' 'NEC_Networks_LLC'\n",
      " 'Neiman_Marcus' 'Nemours_Foundation' 'Network_Solutions'\n",
      " 'New_York_City_Health_&_Hospitals_Corp.' 'New_York_State_Electric_&_Gas'\n",
      " 'New_York_Taxis' 'Nexon_Korea_Corp' 'NHS' 'Nintendo_(Club_Nintendo)'\n",
      " 'Nintendo_(Nintendo_Account)' 'Nippon_Television' 'Nival_Networks'\n",
      " 'Norwegian_Tax_Administration' 'Now:Pensions' 'Ofcom'\n",
      " 'US_Office_of_Personnel_Management'\n",
      " 'Office_of_the_Texas_Attorney_General' 'Ohio_State_University' 'Orbitz'\n",
      " 'Oregon_Department_of_Transportation' 'OVH' 'Patreon' 'PayPay' 'Popsugar'\n",
      " 'Premera' 'Puerto_Rico_Department_of_Health' 'Quest_Diagnostics' 'Quora'\n",
      " 'Rakuten' 'Rambler.ru' 'RBS_Worldpay' 'Reddit' 'Restaurant_Depot'\n",
      " 'RockYou!' 'Rosen_Hotels' 'Sakai_City_Japan'\n",
      " 'San_Francisco_Public_Utilities_Commission' 'Scottrade' 'Scribd'\n",
      " 'Seacoast_Radiology_PA' 'Sega'\n",
      " 'Service_Personnel_and_Veterans_Agency_(UK)' 'ShopBack' 'SingHealth'\n",
      " 'Slack' 'SlickWraps' 'SnapChat' 'Sony_Online_Entertainment'\n",
      " 'Sony_Pictures' 'Sony_PlayStation_Network' 'South_Africa_police'\n",
      " 'South_Carolina_Government' 'South_Shore_Hospital_Massachusetts'\n",
      " 'Southern_California_Medical_Legal_Consultants'\n",
      " 'Spartanburg_Regional_Healthcare_System' 'Stanford_University'\n",
      " 'Starbucks'\n",
      " 'Starwoodincluding_Westin_Hotels_&_Resorts_and_Sheraton_Hotels_and_Resorts'\n",
      " 'State_of_Texas' 'Steam' 'StockX' 'Stratfor' 'Supervalu'\n",
      " 'Sutter_Medical_Center' 'Syrian_government_(Syria_Files)' 'Taobao'\n",
      " 'Taringa!' 'Target_Corporation' 'TaxSlayer.com' 'TD_Bank'\n",
      " 'TerraCom_&_YourTel' 'Tetrad' 'Texas_Lottery'\n",
      " 'Ticketfly_(subsidiary_of_Eventbrite)' 'Tianya_Club' 'TikTok'\n",
      " 'TK__TJ_Maxx' 'T_Mobile_Deutsche_Telekom' 'T_Mobile' 'Tricare'\n",
      " 'Triple_S_Salud_Inc.' 'Truecaller' 'Trump_Hotels' 'Tumblr' 'Twitch'\n",
      " 'Twitter' 'Typeform' 'Uber' 'Ubisoft' 'Ubuntu'\n",
      " 'UCLA_Medical_Center_Santa_Monica' 'UK_Home_Office'\n",
      " 'UK_Ministry_of_Defence' 'UK_Revenue_&_Customs'\n",
      " 'Universiti_Teknologi_MARA' 'Under_Armour'\n",
      " 'University_of_California_Berkeley' 'University_of_Maryland_College_Park'\n",
      " 'University_of_Central_Florida' 'University_of_Miami'\n",
      " 'University_of_Utah_Hospital_&_Clinics'\n",
      " 'University_of_Wisconsin–Milwaukee' 'United_States_Postal_Service' 'UPS'\n",
      " 'U.S._Army' 'U.S._Army(classified_Iraq_War_documents)'\n",
      " 'U.S._Department_of_Defense' 'U.S._Department_of_Veteran_Affairs'\n",
      " 'U.S._federal_government_(2020_United_States_federal_government_data_breach)'\n",
      " 'U.S._law_enforcement_(70_different_agencies)'\n",
      " 'National_Archives_and_Records_Administration_(U.S._military_veterans_records)'\n",
      " 'U.S._government_(United_States_diplomatic_cables_leak)'\n",
      " 'National_Guard_of_the_United_States' 'Vastaamo' 'Verizon_Communications'\n",
      " 'View_Media' 'Virgin_Media' 'Virginia_Department_of_Health'\n",
      " 'Virginia_Prescription_Monitoring_Program' 'Vodafone' 'VTech' 'Walmart'\n",
      " 'Washington_Post' 'Washington_State_court_system' 'Wattpad'\n",
      " 'Wawa_(company)' 'Weebly' \"Wendy's\" 'Westpac' 'Writerspace.com' 'Xat.com'\n",
      " 'Yahoo' 'Yahoo_Japan' 'Yahoo!_Voices' 'Yale_University' 'YouTube'\n",
      " 'Zappos' 'Zynga'\n",
      " 'Unknown_agency(believed_to_be_tied_to_United_States_Census_Bureau)'\n",
      " 'National_Health_Information_Center_(NCZI)_of_Slovakia'\n",
      " '50_companies_and_government_institutions' 'IKEA']\n"
     ]
    }
   ],
   "source": [
    "#convert spaces, /, \",\", and - to _ in Organization type\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace(' ', '_')\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace('/', '_')\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace('-', '_')\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace(',', '_')\n",
    "\n",
    "#convert spaces, /, \",\", __, and - to _ in Entity\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace(' ', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace('/', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace('-', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace(',', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace('__', '_')\n",
    "\n",
    "#print out the unique values in entity\n",
    "print(df_vis['Entity'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are the same entity, such as Yahoo, Sony, Nintendo, British_Airways, Gmail and google, and Adobe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename 'Adobe_Systems_Incorporated and 'Adobe_Inc.' to 'Adobe'\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Adobe_Systems_Incorporated': 'Adobe', 'Adobe_Inc.': 'Adobe'})\n",
    "#Rename \"Yahoo_Japan\" and \"Yahoo!_Voices\" to \"Yahoo\"\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Yahoo_Japan': 'Yahoo', 'Yahoo!_Voices': 'Yahoo'})\n",
    "#Rename \"Sony_Pictures\" and \"Sony_Playstation_Network\" to \"Sony\"\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Sony_Pictures': 'Sony', 'Sony_Playstation_Network': 'Sony'})\n",
    "#Rename \"Nintendo_(Nintendo_Acount)\" and \"Nintendo_(Club_Nintendo) to \"Nintendo\"\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Nintendo_(Nintendo_Acount)': 'Nintendo', 'Nintendo_(Club_Nintendo)': 'Nintendo'})\n",
    "#rename '2018_British_Airways_cyberattack' and 'British_Airways' to 'BritishAirways'\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'2018_British_Airways_cyberattack': 'BritishAirways', 'British_Airways': 'BritishAirways'})\n",
    "#rename 'Gmail' and 'Google_Plus' to 'Google'\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Gmail': 'Google', 'Google_Plus': 'Google'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n"
     ]
    }
   ],
   "source": [
    "#print out the unique values in entity\n",
    "print(len(df_vis['Entity'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healthcare' 'social_networking' 'tech' 'advertising'\n",
      " 'telecommunications' 'transport' 'web' 'financial' 'gaming'\n",
      " 'tech__retail' 'telecoms' 'government' 'academic' 'retail'\n",
      " 'educational_services' 'banking' 'game' 'energy' 'background_check'\n",
      " 'hosting_provider' 'military' 'health' 'messaging_app' 'restaurant'\n",
      " 'financial__credit_reporting' 'data_broker' 'social_network'\n",
      " 'financial_service_company'\n",
      " 'personal_and_demographic_data_about_residents_and_their_properties_of_US'\n",
      " 'hotel' 'humanitarian' 'special_public_corporation' 'shopping'\n",
      " 'local_search' 'mobile_carrier' 'publisher_(magazine)'\n",
      " 'government__healthcare' 'web__military' 'online_shopping' 'genealogy'\n",
      " 'media' 'telecom' 'QR_code_payment' 'fashion' 'Clinical_Laboratory'\n",
      " 'Question_&_Answer' 'web__gaming' 'government__database'\n",
      " 'phone_accessories' 'web__tech' 'market_analysis' 'ticket_distribution'\n",
      " 'social_media' 'military__healthcare' 'Telephone_directory'\n",
      " 'Consumer_Goods' 'government__military' 'online_marketing' 'tech__web'\n",
      " 'various']\n"
     ]
    }
   ],
   "source": [
    "print(df_vis[\"Organization type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healthcare' 'social_networking' 'tech' 'advertising'\n",
      " 'telecommunications' 'transport' 'web' 'financial' 'gaming' 'tech_retail'\n",
      " 'telecoms' 'government' 'academic' 'retail' 'educational_services'\n",
      " 'banking' 'game' 'energy' 'background_check' 'hosting_provider'\n",
      " 'military' 'health' 'messaging_app' 'restaurant'\n",
      " 'financial_credit_reporting' 'data_broker' 'social_network'\n",
      " 'financial_service_company'\n",
      " 'personal_and_demographic_data_about_residents_and_their_properties_of_US'\n",
      " 'hotel' 'humanitarian' 'special_public_corporation' 'shopping'\n",
      " 'local_search' 'mobile_carrier' 'publisher_(magazine)'\n",
      " 'government_healthcare' 'web_military' 'online_shopping' 'genealogy'\n",
      " 'media' 'telecom' 'QR_code_payment' 'fashion' 'Clinical_Laboratory'\n",
      " 'Question_&_Answer' 'web_gaming' 'government_database'\n",
      " 'phone_accessories' 'web_tech' 'market_analysis' 'ticket_distribution'\n",
      " 'social_media' 'military_healthcare' 'Telephone_directory'\n",
      " 'Consumer_Goods' 'government_military' 'online_marketing' 'tech_web'\n",
      " 'various']\n"
     ]
    }
   ],
   "source": [
    "#replace \"__\" with \"_\"\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace('__', '_')\n",
    "print(df_vis[\"Organization type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healthcare' 'social' 'tech' 'advertising' 'telecommunications'\n",
      " 'transport' 'web' 'financial' 'gaming' 'retail' 'government' 'education'\n",
      " 'energy' 'demographic' 'military' 'restaurant' 'hotel' 'media'\n",
      " 'government_healthcare' 'genealogy' 'financial_retail_tech'\n",
      " 'clinical_laboratory' 'military_healthcare' 'government_military'\n",
      " 'tech_web' 'various']\n"
     ]
    }
   ],
   "source": [
    "#for each row in the data frame, \n",
    "for index, row in df_vis.iterrows():\n",
    "#convert the organization type to lower case\n",
    "    df_vis.loc[index, 'Organization type'] = row['Organization type'].lower()\n",
    "# if the organization type is web_ and something else, replace it with the last word. \n",
    "    if row['Organization type'].split('_')[0] == 'web' and len(row['Organization type'].split('_'))>1:\n",
    "        df_vis.loc[index, 'Organization type'] = row['Organization type'].split('_')[-1]\n",
    "#if the organization type is 'health', replace it with 'healthcare'\n",
    "    elif row['Organization type'] == 'health':\n",
    "        df_vis.loc[index, 'Organization type'] = 'healthcare'\n",
    "# if it is telecoms or telecom, replace it with telecommunications\n",
    "    elif row['Organization type'] == 'telecoms' or row['Organization type'] == 'telecom':\n",
    "        df_vis.loc[index, 'Organization type'] = 'telecommunications'\n",
    "#if the organization type is banking, or the first word in the organization type is 'financial', replace it with 'financial'\n",
    "    elif row['Organization type'].split('_')[0] == 'financial' or row['Organization type'] == 'banking':\n",
    "        df_vis.loc[index, 'Organization type'] = 'financial'\n",
    "#if the first word in the organization type is 'social', replace it with 'social'\n",
    "    elif row['Organization type'].split('_')[0] == 'social':\n",
    "        df_vis.loc[index, 'Organization type'] = 'social'\n",
    "#if the first word in the organization type is 'Telephone', replace it with 'telecomunications'\n",
    "    elif row['Organization type'].split('_')[0] == 'Telephone' or row['Organization type'].split('_')[0] == 'telephone':\n",
    "        df_vis.loc[index, 'Organization type'] = 'telecommunications'\n",
    "#if there are two words and the first word in the organization type is 'web', and the second word is not 'tech', replace it with 'the second word\n",
    "    elif (row['Organization type'].split('_')[0] == 'web' and len(row['Organization type'].split('_'))>1 and row['Organization type'].split('_')[-1] != 'tech') or (row['Organization type'] == 'hosting_provider'):\n",
    "        df_vis.loc[index, 'Organization type'] = 'web'\n",
    "#if the first word in the organization type is 'mobile', replace it with 'telecomunications'\n",
    "    elif row['Organization type'].split('_')[0] == 'mobile':\n",
    "        df_vis.loc[index, 'Organization type'] = 'telecommunications'\n",
    "#if the first word is shopping or Consumer, replace it with 'retail'\n",
    "    elif row['Organization type'].split('_')[0] == 'shopping' or row['Organization type'].split('_')[0] == 'Consumer':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the first word is tech, and there is a second word that is retail, replace it with retail\n",
    "    elif row['Organization type'].split('_')[0] == 'tech' and len(row['Organization type'].split('_'))>1 and row['Organization type'].split('_')[-1] == 'retail':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the first word is game, replace it with 'gaming'\n",
    "    elif row['Organization type'].split('_')[0] == 'game':\n",
    "        df_vis.loc[index, 'Organization type'] = 'gaming'\n",
    "#if the organization type is \"phone_accessories\", replace it with \"retail\"\n",
    "    elif row['Organization type'] == 'phone_accessories':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the first word is \"personal\", \"humanitarian, \"special\", or \"local\", replace it with \"demographic\"\n",
    "    elif row['Organization type'].split('_')[0] == 'personal' or row['Organization type'].split('_')[0] == 'humanitarian' or row['Organization type'].split('_')[0] == 'special' or row['Organization type'].split('_')[0] == 'local':\n",
    "        df_vis.loc[index, 'Organization type'] = 'demographic'\n",
    "# if the first word is educational or academic, replace it with \"education\"\n",
    "    elif row['Organization type'].split('_')[0] == 'educational' or row['Organization type'].split('_')[0] == 'academic':\n",
    "        df_vis.loc[index, 'Organization type'] = 'education'\n",
    "#if the first word is messaging, replace it with \"social\"\n",
    "    elif row['Organization type'].split('_')[0] == 'messaging':\n",
    "        df_vis.loc[index, 'Organization type'] = 'social'\n",
    "#if the organization type is online_shopping, replac eit with retail\n",
    "    elif row['Organization type'] == 'online_shopping':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the type is market_analysis, ticket_distribution, or online_marketing, replace it with \"demographic\"\n",
    "    elif row['Organization type'] == 'market_analysis' or row['Organization type'] == 'ticket_distribution' or row['Organization type'] == 'online_marketing':\n",
    "        df_vis.loc[index, 'Organization type'] = 'demographic'\n",
    "#if the first word is \"publisher\", replace it with \"media\"\n",
    "    elif row['Organization type'].split('_')[0] == 'publisher':\n",
    "        df_vis.loc[index, 'Organization type'] = 'media'\n",
    "#if the organization type is QR_code_payment, replace it with \"financial_retail_tech\"\n",
    "    elif row['Organization type'] == 'QR_code_payment':\n",
    "        df_vis.loc[index, 'Organization type'] = 'financial_retail_tech'\n",
    "#if the organization type is 'fashion', replace it with \"retail\"\n",
    "    elif row['Organization type'] == 'fashion' or row['Organization type'] == 'consumer_goods':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the organization type is background_check, replace it with \"demographic\"\n",
    "    elif row['Organization type'] == 'background_check' or row['Organization type'] == 'data_broker' or row['Organization type'] == 'question_&_answer':\n",
    "        df_vis.loc[index, 'Organization type'] = 'demographic'\n",
    "#if the organization type is government_database, replace it with \"government\"\n",
    "    elif row['Organization type'] == 'government_database':\n",
    "        df_vis.loc[index, 'Organization type'] = 'government'\n",
    "#replace \"question_&_answer\" with \"demographic\"\n",
    "if df_vis['Organization type'].str.contains('question_&_answer').any():\n",
    "    df_vis['Organization type'] = df_vis['Organization type'].str.replace('question_&_answer', 'demographic')\n",
    "print(df_vis[\"Organization type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa512522e893902e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.878799600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizations:  26\n"
     ]
    }
   ],
   "source": [
    "#count the unique values in the organization column\n",
    "print(\"Organizations: \", len(df_vis['Organization type'].unique()))  # there are 60 values in the organization column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizations:  22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_rows = []\n",
    "\n",
    "for index, row in df_vis.iterrows():\n",
    "    if row['Organization type'] == \"government_healthcare\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'government'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'healthcare'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "    elif row['Organization type'] == \"military_healthcare\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'military'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'healthcare'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "    elif row['Organization type'] == \"government_military\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'government'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'military'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "    elif row['Organization type'] == \"tech_web\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'tech'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'web'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "# Concatenate the new rows with the original DataFrame\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "df_vis = pd.concat([df_vis, new_rows_df], ignore_index=True)\n",
    "print(\"Organizations: \", len(df_vis['Organization type'].unique()))  # there are 60 values in the organization column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no null values in the dataframe\n",
      "Entity                object\n",
      "Year                   int64\n",
      "Records              float64\n",
      "Organization type     object\n",
      "Method                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check if there are any null values in each column\n",
    "if df_vis.isnull().values.any():\n",
    "    print(\"There are null values in the dataframe\")\n",
    "else:\n",
    "    print(\"There are no null values in the dataframe\")\n",
    "\n",
    "print(df_vis.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Year:  2004\n",
      "Highest Year:  2022\n",
      "Lowest Records:  6700.0\n",
      "Highest Records:  3000000000.0\n",
      "\n",
      "Organization type\n",
      "web          5.640594e+09\n",
      "financial    1.912284e+09\n",
      "social       1.460870e+09\n",
      "Name: Records, dtype: float64\n",
      "\n",
      "Total records lost in the healthcare industry: \n",
      "182332465.0\n"
     ]
    }
   ],
   "source": [
    "#print the lowest and highest year\n",
    "print(\"Lowest Year: \", df_vis['Year'].min())\n",
    "print(\"Highest Year: \", df_vis['Year'].max())\n",
    "\n",
    "# print the lowest and highest records\n",
    "print(\"Lowest Records: \", df_vis['Records'].min())\n",
    "print(\"Highest Records: \", df_vis['Records'].max())\n",
    "print()\n",
    "\n",
    "#print the top 3 organizations with the highest records\n",
    "print(df_vis.groupby('Organization type')['Records'].sum().sort_values(ascending=False).head(3))\n",
    "print()\n",
    "\n",
    "#print the total records for the healthcare industry\n",
    "print(\"Total records lost in the healthcare industry: \")\n",
    "print(df_vis[df_vis['Organization type'] == 'healthcare']['Records'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Entity  Year      Records  \\\n",
      "0                                21st_Century_Oncology  2016    2200000.0   \n",
      "1                                                500px  2020   14870304.0   \n",
      "2                                Accendo_Insurance_Co.  2020     175350.0   \n",
      "3                                                Adobe  2013  152000000.0   \n",
      "4                                                Adobe  2019    7500000.0   \n",
      "..                                                 ...   ...          ...   \n",
      "346                                            Tricare  2011    2450716.0   \n",
      "347                 U.S._Department_of_Veteran_Affairs  2006   13250000.0   \n",
      "348  U.S._federal_government_(2020_United_States_fe...  2020   13250000.0   \n",
      "349                      Virginia_Department_of_Health  2009    4128689.0   \n",
      "350                                              Yahoo  2013   11000000.0   \n",
      "\n",
      "    Organization type                  Method  \n",
      "0          healthcare                  hacked  \n",
      "1              social                  hacked  \n",
      "2          healthcare           poor security  \n",
      "3                tech                  hacked  \n",
      "4                tech           poor security  \n",
      "..                ...                     ...  \n",
      "346          military  lost / stolen computer  \n",
      "347        government  lost / stolen computer  \n",
      "348        government                  hacked  \n",
      "349        government                  hacked  \n",
      "350              tech                  hacked  \n",
      "\n",
      "[351 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hacked',\n",
       " 'poor_security',\n",
       " 'lost_stolen_media',\n",
       " 'accidentally_published',\n",
       " 'inside_job,_hacked',\n",
       " 'lost_stolen_computer',\n",
       " 'unsecured_s3_bucket',\n",
       " 'unknown',\n",
       " 'inside_job',\n",
       " 'accidentally_uploaded',\n",
       " 'poor_security_hacked',\n",
       " 'unprotected_api',\n",
       " 'poor_security_inside_job',\n",
       " 'data_exposed_by_misconfiguration',\n",
       " 'intentionally_lost',\n",
       " 'misconfiguration_poor_security',\n",
       " 'ransomware_hacked',\n",
       " 'rogue_contractor',\n",
       " 'improper_setting,_hacked',\n",
       " 'hacked_misconfiguration',\n",
       " 'publicly_accessible_amazon_web_services_(aws)_server',\n",
       " 'accidentally_exposed',\n",
       " 'social_engineering']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vis)\n",
    "# convert all uppercase letters to lowercase\n",
    "df_vis['Method'] = df_vis['Method'].str.lower()\n",
    "\n",
    "# replace all spaces with \"_\"\n",
    "df_vis['Method'] = df_vis['Method'].str.replace(' ', '_')\n",
    "\n",
    "# replace all slash signs with \"_\"\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('/', '_')\n",
    "\n",
    "# replace all __ and ___ with _\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('___', '_')\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('__', '_')\n",
    "\n",
    "# check the updated dataframe\n",
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_vis.iterrows():\n",
    "    if \"poor_security\" in row[\"Method\"] or \"unsecured_s3_bucket\" in row[\"Method\"] or \"poor_security_hacked\" in row[\"Method\"] or \"misconfiguration\" in row[\"Method\"] or \"improper_setting\" in row[\"Method\"] or \"publicly_accessible\" in row[\"Method\"] or \"unprotected_api\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"poor_security\"\n",
    "    elif \"lost_stolen_media\" in row[\"Method\"] or \"lost_stolen_computer\" in row[\"Method\"]: \n",
    "        df_vis.loc[index, \"Method\"] = \"stolen_media\"\n",
    "    elif \"_hacked\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"hacked\"\n",
    "    elif \"intentionally_lost\" in row[\"Method\"] or \"accidentally_exposed\"  in row[\"Method\"]  or \"accidentally_published\" in row[\"Method\"] or \"accidentally_uploaded\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"accidental_breach\"\n",
    "    elif \"inside_job\" in row[\"Method\"] or \"rogue_contractor\" in row[\"Method\"] or \"social_engineering\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"inside_job\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of records for each method (excluding \"unknown\")\n",
    "method_records = df_vis[df_vis['Method'] != 'unknown'].groupby('Method')['Records'].sum()\n",
    "\n",
    "# Calculate the total sum of records (excluding \"unknown\")\n",
    "total_records = method_records.sum()\n",
    "\n",
    "# Calculate the percentage of records lost for each method (excluding \"unknown\")\n",
    "method_percentages = (method_records / total_records) * 100\n",
    "\n",
    "# Print out the percentages\n",
    "print(\"Percentage of records lost for each method:\")\n",
    "print(method_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_vis.copy()\n",
    "# rename unknown columns to poor_security\n",
    "df_class['Method'] = df_class['Method'].replace({'unknown': 'poor_security'})\n",
    "print(df_class['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'] = df_vis['Method'].replace({'accidental_breach': 'poor_security', 'inside_job': 'hacked', 'stolen_media': 'poor_security'})\n",
    "print(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of records for the \"unknown\" method\n",
    "unknown_records = df_vis[df_vis['Method'] == 'unknown']['Records'].sum()\n",
    "\n",
    "# Calculate the total sum of records\n",
    "total_records = df_vis['Records'].sum()\n",
    "\n",
    "# Calculate the percentage of \"unknown\" method records lost\n",
    "unknown_percentage = (unknown_records / total_records) * 100\n",
    "\n",
    "# Print the percentage\n",
    "print(\"Percentage of 'unknown' method records lost: \", unknown_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'] = df_vis['Method'].replace('unknown', 'poor_security')\n",
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vis.drop(\"Entity\", axis=1, inplace=True)\n",
    "df_vis[\"Records\"] = df_vis[\"Records\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis.dtypes)\n",
    "print(df_class.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#convert the years to ints\n",
    "df_vis['Year'] = df_vis['Year'].astype(int)\n",
    "# Grouping the data by year\n",
    "grouped_data = df_vis.groupby('Year').size()\n",
    "\n",
    "# Sorting the grouped data by year\n",
    "grouped_data = grouped_data.sort_index()\n",
    "\n",
    "# Convert the years to whole numbers\n",
    "grouped_data.index = grouped_data.index.astype(int)\n",
    "\n",
    "# Creating a bar plot with figsize\n",
    "fig, ax = plt.subplots(figsize=(25, 12))\n",
    "\n",
    "# Use a colormap to create a gradient effect\n",
    "norm = plt.Normalize(0, grouped_data.max())\n",
    "colors = plt.cm.Reds(norm(grouped_data.values))\n",
    "bars = ax.bar(grouped_data.index, grouped_data.values, color=colors)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Entries')\n",
    "ax.set_title('Number of Entries per Year')\n",
    "\n",
    "# Add a colorbar to show the scale\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='Reds'), ax=ax, orientation='vertical')\n",
    "cbar.set_label('Record Count')\n",
    "\n",
    "# Make the background transparent\n",
    "# fig.patch.set_alpha(0.0)\n",
    "# ax.patch.set_alpha(0.0)\n",
    "\n",
    "# Save the plot with a transparent background\n",
    "plt.savefig('graphs/entries.png', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and sum the records for each year\n",
    "data_lost_over_time = df_vis.groupby('Year')['Records'].sum()\n",
    "\n",
    "# Create a line plot for data lost over time\n",
    "data_lost_over_time.plot(kind='line', figsize=(25, 12))\n",
    "plt.title(\"Data Lost Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Data Lost\")\n",
    "#save the plot as \"records_line_plot.png\" in graphs\n",
    "plt.savefig('graphs/records_line_plot.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by industry and sum the records for each industry, then select the top 20 industries\n",
    "top_20_industries = df_vis.groupby('Organization type')['Records'].sum().nlargest(20)\n",
    "\n",
    "# Create a bar plot for the top 10 industries with the highest data loss\n",
    "top_20_industries.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 20 Industries with Highest Data Loss\")\n",
    "plt.xlabel(\"Industry\")\n",
    "plt.ylabel(\"Total Data Lost\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the method and count the occurrences, then select the top 10 causes\n",
    "top_10_causes = df_vis['Method'].value_counts().nlargest(10)\n",
    "\n",
    "# Create a bar plot for the top 10 causes of data breaches\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 10 Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12), yerr=df_vis['Method'].value_counts().std())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_causes = df_class['Method'].value_counts()\n",
    "\n",
    "# Set up a color palette with a unique color for each bar\n",
    "colors = sns.color_palette('viridis', len(top_causes))\n",
    "top_limit = top_causes.max() + 20\n",
    "plt.ylim(bottom=0, top = top_limit)\n",
    "# Create a bar plot for the causes of data breaches\n",
    "top_causes.plot(kind='bar', figsize=(12, 6), color=colors, yerr=df_class['Method'].value_counts().std())\n",
    "plt.title(\"Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add labels to each bar containing the count of entries for each cause. offset the labels to the right by 0.1\n",
    "for index, value in enumerate(top_causes):\n",
    "    plt.text(index + 0.145, value + 1, str(value), ha='center', va='bottom' , rotation=45, fontweight='bold', color='black', fontsize=12)\n",
    "\n",
    "\n",
    "#save the plot as \"breachcauses.png\" in the directory called graphs\n",
    "plt.savefig('graphs/breachcauses.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we calculate the error bars, theres an upper and lower bound, the upper bound is the mean + the standard deviation, the lower bound is the mean - the standard deviation.\n",
    "So, drawing a line through the lower bound, and it doesnt touch the other line, that means that the difference is statistically significant.\n",
    "\n",
    "We may want to exclude the data that is statistically significant, because it may be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(20).index\n",
    "\n",
    "# Filtering the DataFrame to include only the top 20 industries\n",
    "records_by_year_and_industry_top20 = records_by_year_and_industry[top_20_industries]\n",
    "\n",
    "# Creating a stacked bar plot for records by year and industry (top 20)\n",
    "records_by_year_and_industry_top20.plot(kind='bar', stacked=True, figsize=(25, 12))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title(\"Stacked Bar Plot of Records by Year and Top 20 Industries\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Records\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='Industry Type', bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\n",
    "\n",
    "#save the plot as \"industryrecords.png\" in the directory called graphs\n",
    "plt.savefig('graphs/industryrecords.png', bbox_inches='tight')\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory to save the plots\n",
    "output_directory = 'graphs'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "organization_types = df_class['Organization type'].unique()\n",
    "# Iterate over the organization types and create a pie chart for each\n",
    "for org_type in organization_types:\n",
    "    # Filter the data for the current organization type\n",
    "    org_data = df_class[df_class['Organization type'] == org_type]\n",
    "    \n",
    "    # Get the unique methods for the current organization type\n",
    "    methods = org_data['Method'].unique()\n",
    "    \n",
    "    # Get the counts for each method\n",
    "    method_counts = org_data['Method'].value_counts()\n",
    "    \n",
    "    # Generate a list of colors for the pie chart\n",
    "    colors = plt.cm.Set3(range(len(methods)))\n",
    "    \n",
    "    # Create the pie chart\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.pie(method_counts, labels=methods, colors=colors, autopct='%1.1f%%')\n",
    "    plt.title(org_type)\n",
    "    #make the background transparent\n",
    "    plt.gca().patch.set_alpha(0.0)\n",
    "    #make the text larger and bold\n",
    "    plt.rcParams.update({'font.size': 18, 'font.weight': 'bold'}) \n",
    "    # Save the plot as a .png file in the specified directory\n",
    "    output_filename = os.path.join(output_directory, f'{org_type}_breach_causes.png')\n",
    "    #make the background transparent\n",
    "    plt.savefig(output_filename, transparent=True)\n",
    "    #plt.savefig(output_filename)\n",
    "    plt.show()\n",
    "    # Close the current plot to free up resources for the next iteration\n",
    "    plt.close()\n",
    "\n",
    "print(\"Plots saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(13).index\n",
    "\n",
    "# Filtering the DataFrame to include all industries and filling NaN values with 0\n",
    "records_by_year_and_industry_all = records_by_year_and_industry.fillna(0)\n",
    "\n",
    "# Create a consolidated image for all pie charts\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Create subplots based on the number of years\n",
    "num_years = len(records_by_year_and_industry_all.index)\n",
    "num_cols = 4 \n",
    "num_rows = (num_years + num_cols - 1) // num_cols\n",
    "\n",
    "\n",
    "for i, year in enumerate(records_by_year_and_industry_all.index, start=1):\n",
    "    data_for_year = records_by_year_and_industry_all.loc[year]\n",
    "\n",
    "    # Calculate the percentage of records lost in the healthcare industry and the rest\n",
    "    #the total number of records lost that year where \"Organization type == healthcare\"\n",
    "    healthcare_sum = data_for_year.groupby('Organization type').sum()['healthcare']\n",
    "    healthcare_percentage = healthcare_sum / data_for_year.sum()\n",
    "    other_percentage = 1 - healthcare_percentage\n",
    "\n",
    "    # Create subplots\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.pie([healthcare_percentage, other_percentage], labels=['healthcare', 'other'],\n",
    "            autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "            startangle=90)\n",
    "    plt.title(f\"{year}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(13).index\n",
    "\n",
    "# Filtering the DataFrame to include all industries and filling NaN values with 0\n",
    "records_by_year_and_industry_all = records_by_year_and_industry.fillna(0)\n",
    "wp = { 'linewidth' : 1, 'edgecolor' : \"black\" }\n",
    "# Creating explode data\n",
    "explode = (0.1, 0.0)\n",
    "# Create subplots for each year\n",
    "for i, year in enumerate(records_by_year_and_industry_all.index, start=1):\n",
    "    # Create a separate figure for each year\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    data_for_year = records_by_year_and_industry_all.loc[year]\n",
    "\n",
    "    # Calculate the percentage of records lost in the healthcare industry and the rest\n",
    "    # the total number of records lost that year where \"Organization type == healthcare\"\n",
    "    healthcare_sum = data_for_year.groupby('Organization type').sum().get('healthcare', 0)\n",
    "    healthcare_percentage = healthcare_sum / data_for_year.sum()\n",
    "    if healthcare_percentage > 0.02:\n",
    "        \n",
    "        other_percentage = 1 - healthcare_percentage\n",
    "\n",
    "        # Create subplots\n",
    "        plt.pie([healthcare_percentage, other_percentage], labels=['Healthcare', 'Other'],\n",
    "                autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '', wedgeprops=wp, explode = explode,\n",
    "                colors = ['dodgerblue', 'gold'],\n",
    "                startangle=90)\n",
    "        plt.title(f\"{year}\")\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(healthcare_sum)\n",
    "        plt.gcf()\n",
    "        #save the plot as \"healthcare_[year].png\" in the directory called graphs]\" with a transparent background\n",
    "        plt.savefig(f'graphs/healthcare_{year}.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis)\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()  #perform label encoding on the organization_type, the copy of df_vis\n",
    "\n",
    "# implement label encoding on the Organization type column\n",
    "le.fit(df_vis['Organization type'])   # fit the data we want to train the encoder on\n",
    "df_vis['Organization type'] = le.transform(df_vis['Organization type'])\n",
    "# observe how the column 'Organization Type' has changed\n",
    "df_vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.dtypes   # as we can see, the Organization type changed from Object --> integer datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the label frequency, to gain an understanding of outliers and inlier values\n",
    "df_vis['Organization type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'].value_counts()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which columns have null/unknoown values\n",
    "null_mask = df_vis.isnull().any(axis=1)\n",
    "null_rows=df_vis[null_mask]\n",
    "\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.isnull().all()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_vis['Method'].unique()))  \n",
    "print(len(list(df_vis['Method'].value_counts())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le.fit(df_vis['Method'])  # train the label encoder on the column data we want to train\n",
    "df_vis['Method'] = le.transform(df_vis['Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(df_vis['Entity'])  # train the label encoder on the column data we want to train\n",
    "df_vis['Entity'] = le.transform(df_vis['Entity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READY TO TRAIN – Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Organization type', 'Year', 'Records', 'Entity']\n",
    "label = 'Method'\n",
    "\n",
    "X = df_vis[features]\n",
    "y = df_vis[label]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_standardized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.to_numpy(), dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Define the neural network model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_splits = 15\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the performance metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "#int sum for average accuracy\n",
    "sum_accuracy = 0\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tensor)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_tensor[train_index], X_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_tensor[train_index], y_tensor[val_index]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for start in range(0, len(X_train_fold), batch_size):\n",
    "            X_batch = X_train_fold[start:start+batch_size]\n",
    "            y_batch = y_train_fold[start:start+batch_size]\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model(X_val_fold)\n",
    "        mse_val = loss_fn(y_pred_val, y_val_fold)\n",
    "        fold_metrics.append(mse_val.item())\n",
    "        y_pred_binary = (y_pred_val >= 0.5).float()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (y_pred_binary == y_val_fold).sum().item()\n",
    "        total = y_val_fold.size(0)\n",
    "        accuracy = correct / total * 100.0\n",
    "        sum_accuracy+=accuracy\n",
    "\n",
    "        print(f\"Fold {fold + 1} - Validation MSE: {mse_val:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "print(f\"Average accuracy: {sum_accuracy/n_splits:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization on multiclass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by organization type and method, and sum the records for each combination\n",
    "grouped_data = df_class.groupby(['Organization type', 'Method'])['Records'].sum().unstack()\n",
    "\n",
    "# Create a stacked bar plot\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Records by Organization Type and Method')\n",
    "plt.xlabel('Organization Type')\n",
    "plt.ylabel('Records')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network – Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_class.drop(\"Entity\", axis=1, inplace=True)\n",
    "df_class[\"Records\"] = df_class[\"Records\"].astype(int)\n",
    "# Group the data by the method and count the occurrences, then select the top 10 causes\n",
    "top_10_causes = df_class['Method'].value_counts().nlargest(10)\n",
    "\n",
    "# Create a bar plot for the top 10 causes of data breaches\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 10 Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12), yerr=df_class['Method'].value_counts().std())\n",
    "plt.show()\n",
    "\n",
    "print(df_class['Method'].unique())\n",
    "print(df_class['Method'].value_counts())\n",
    "\n",
    "le = LabelEncoder()  #perform label encoding on the organization_type, the copy of df_vis\n",
    "\n",
    "# implement label encoding on the Organization type column\n",
    "le.fit(df_class['Organization type'])   # fit the data we want to train the encoder on\n",
    "df_class['Organization type'] = le.transform(df_class['Organization type'])\n",
    "le.fit(df_class['Method'])  # train the label encoder on the column data we want to train\n",
    "df_class['Method'] = le.transform(df_class['Method'])\n",
    "le.fit(df_class['Entity'])  # train the label encoder on the column data we want to train\n",
    "df_class['Entity'] = le.transform(df_class['Entity'])\n",
    "print(df_class.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(4, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 45),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(45, 5)\n",
    "    \n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.0001)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 75\n",
    "batch_size = 1\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_splits = 16\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the performance metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "sum_accuracy2 = 0\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tensor)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_tensor[train_index], X_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_tensor[train_index], y_tensor[val_index]\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 2  # Number of epochs with no improvement after which training will be stopped\n",
    "    count = 0  # Counter for patience\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model2.train()\n",
    "        for start in range(0, len(X_train_fold), batch_size):\n",
    "            X_batch = X_train_fold[start:start+batch_size]\n",
    "            y_batch = y_train_fold[start:start+batch_size]\n",
    "\n",
    "            y_pred = model2(X_batch)\n",
    "            \n",
    "            # Ensure that y_batch contains class indices\n",
    "            y_batch = y_batch.long().squeeze(dim=1)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model2(X_val_fold)\n",
    "        \n",
    "        # Ensure that y_val_fold contains class indices\n",
    "        y_val_fold = y_val_fold.long().squeeze(dim=1)\n",
    "\n",
    "        loss_val = loss_fn(y_pred_val, y_val_fold)\n",
    "        fold_metrics.append(loss_val.item())\n",
    "\n",
    "        # Convert logits to predicted class\n",
    "        y_pred_classes = torch.argmax(y_pred_val, dim=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (y_pred_classes == y_val_fold).sum().item()\n",
    "        total = y_val_fold.size(0)\n",
    "        accuracy = correct / total * 100.0\n",
    "        sum_accuracy2+=accuracy\n",
    "        print(f\"Fold {fold + 1} - Validation Loss: {loss_val:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        if loss_val < best_val_loss:\n",
    "                best_val_loss = loss_val\n",
    "                count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        if count == patience:\n",
    "            print(\"Early stopping. No improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "print(f\"Average accuracy: {sum_accuracy2/n_splits:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Digraph object with left-to-right layout\n",
    "dot = Digraph(comment='Neural Network Model', format='png')\n",
    "dot.attr(rankdir='LR')\n",
    "\n",
    "# Add nodes and edges for each layer\n",
    "for i, layer in enumerate(model2):\n",
    "    if i == 0:\n",
    "        layer_name = \"Input Layer\"\n",
    "        label = f\"Input Layer \\n{layer.in_features} Features\"\n",
    "    elif i == len(model2) - 1:\n",
    "        layer_name = \"Output Layer\"\n",
    "        label = f\"Output Layer \\n{layer.out_features} Outputs\"\n",
    "    else:\n",
    "        layer_name = f\"Layer_{i}\"\n",
    "        # Check the layer type and label accordingly\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            label = f\"Hidden Layer {i // 2}\"\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            label = \"ReLU\\nActivation Function\"\"\n",
    "\n",
    "    dot.node(layer_name, label=label)\n",
    "\n",
    "    if i > 1:\n",
    "        dot.edge(f\"Layer_{i-1}\", layer_name)\n",
    "    if i ==1:\n",
    "        dot.edge(\"Input Layer\", layer_name)\n",
    "\n",
    "# Save and render the graph\n",
    "dot.render(\"neural_network_graph_custom_labeled\", cleanup=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification: Optimize number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare the data\n",
    "features = ['Organization type', 'Year', 'Records']\n",
    "label = 'Method'\n",
    "\n",
    "X = df_vis[features]\n",
    "y = df_vis[label]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (20,), (30,), (40,), (50,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='roc_auc', cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and its AUC score on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"AUC Score:\", auc_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
