{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:31.132750700Z",
     "start_time": "2023-09-09T19:20:31.117340600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a5b2fc17a89d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some insights into this project:\n",
    "- The category of machine learning models that these fall into is _supervised learning_. Supervised learning is a type of machine learning where the model is trained on labeled data. The label in this case is the method of data breach.\n",
    "\n",
    "The models that will be tested out will fall into the following category of machine learning:\n",
    "* <u><b>Logistic Regression</b></u>: This is a supervised Learning that can be used to predict a categorical outcome. In this case, the categorical outcome is the method of the data breach. The features that will be used are: entity, year, records and organization type.\n",
    "* <u><b>Decision Trees</b></u>: This is another supervised learning algorithm that can be used to predict a categorical outcome. Decision trees work by creating a tree-like structure that represents the relationships between the features and the outcomes.\n",
    "* <u><b>Support Vector Machine(SVMs):</u></b> This is a supervised learning algorithm that can be used to predict both categorical and continous outcomes. SVMs work by finding the hyperplane that best seperates the data points into different classes.\n",
    "* <u><b>Random Forests</u></b> This is an ensemble learning algorithm that combines multiple decision trees to improve the accuracy of the predictions.\n",
    "* <u><b>Neural Networks</u></b> This is a more complex algorithm that can be used to predict both categorical and continuous outcomes. Neural Networks work by learning the relationships between the features and the outcome through a process called backpropogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a0b8ccba56a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:31.530128100Z",
     "start_time": "2023-09-09T19:20:31.452203700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the dataset(s) we will be using\n",
    "csv_file_path = os.path.abspath('df_1.csv');  # get the absolute path of the CS\n",
    "\n",
    "df = pd.read_csv(csv_file_path);   # Read the CSV file into a datafram\n",
    "# display the head to see if the dataset works as intended\n",
    "df.head(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1621f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes  # simply lists out the datatype we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9e109abfaf428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:33.325609Z",
     "start_time": "2023-09-09T19:20:33.265630700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns # observe the column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc115ab662e2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:33.816103200Z",
     "start_time": "2023-09-09T19:20:33.785937900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will need to rename the unnamed column\n",
    "df.rename(columns={'Unnamed: 0' : 'Index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20893287fb448ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:35.163638500Z",
     "start_time": "2023-09-09T19:20:35.143525800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test out the number of unique values contained in Entity\n",
    "len(df['Entity'].unique())   #there's a total of 331 unique name for comapnies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84958bb201b54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:37.803496900Z",
     "start_time": "2023-09-09T19:20:37.688758800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1 = df.copy()  # we don't want to make modifications to the original dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603b602fd5debd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.813591Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_copy1['Organization type'].unique())   # in terms of organization type, there's 70 different unique values\n",
    "df_copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc8a351025f36a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.833091200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the special characters\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace(',', '_')  # replace the cases of ',' with '_'.\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace(' ', '_') # we also replace the cases of spacing with '_'.\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace('__', '_')  # replace all instances of __ with _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdeb53",
   "metadata": {},
   "source": [
    "Before encoding the organization type, I've copied the df to keep the original data intact. I'm dropping the hashed_Entity column as it is not needed for visualizing â€“ we want to focus on industries and not individual companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce8d0263117907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.847272400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_vis = df.copy()  #create another copy of the df_copy and perform further data preprocessing\n",
    "#drop the Sources column\n",
    "df_vis.drop(columns={'Sources'}, inplace=True)\n",
    "df_vis.drop(columns={'Index'}, inplace=True)\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store index positions and non-integer values as tuples\n",
    "non_integer_records = []\n",
    "\n",
    "# Initialize a set to store unique industries\n",
    "unique_industries = set()\n",
    "\n",
    "# Iterate through the 'Records' column and collect non-integer values with their index positions and industry\n",
    "for index, (value, industry) in enumerate(zip(df_vis['Records'], df_vis['Organization type'])):\n",
    "    try:\n",
    "        int_value = int(value)\n",
    "    except ValueError:\n",
    "        non_integer_records.append((index, str(value), industry))\n",
    "        unique_industries.add(industry)\n",
    "\n",
    "# Print the index positions, values, and industries for non-integer values\n",
    "if non_integer_records:\n",
    "    print(\"Non-integer values in the 'Records' column:\")\n",
    "    print(\"Index, Value, Industry\")\n",
    "    for index, value, industry in non_integer_records:\n",
    "        print(index, value, industry)\n",
    "else:\n",
    "    print(\"No non-integer values in the 'Records' column.\")\n",
    "    \n",
    "# Print the list of unique industries from non-integer records\n",
    "if unique_industries:\n",
    "    print(\"Unique industries from non-integer records:\")\n",
    "    print(list(unique_industries))\n",
    "else:\n",
    "    print(\"No unique industries found in non-integer records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_non_integer_records = []\n",
    "# types = list()\n",
    "\n",
    "for industry in unique_industries:\n",
    "    # Create a separate DataFrame for the current industry\n",
    "    industry_df = df_vis[df_vis['Organization type'] == industry]\n",
    "    \n",
    "    # Drop entries with non-integer values in 'Records'\n",
    "    industry_df['Records'] = pd.to_numeric(industry_df['Records'], errors='coerce')\n",
    "    industry_df = industry_df.dropna()\n",
    "    \n",
    "    # Calculate the mean records for the current industry\n",
    "    mean_records = industry_df['Records'].mean()\n",
    "    \n",
    "    # #add the type of mean_records to the types list\n",
    "    # types.append(type(mean_records))\n",
    "    \n",
    "    # Update 'Records' in the original DataFrame if mean is not NaN and remove from non_integer_records\n",
    "    #if the mean_record is nan, its type will be float\n",
    "    if type(mean_records) != float:\n",
    "        # Print the mean records for the current industry\n",
    "        # print(f\"Industry: {industry}, Type: {type(mean_records)}, Mean Records: {mean_records}\")\n",
    "        for index, value, industry_name in non_integer_records:\n",
    "            if industry_name == industry:\n",
    "                df_vis.at[index, 'Records'] = mean_records\n",
    "    else:\n",
    "        updated_non_integer_records.append(industry)\n",
    "\n",
    "#make the types list a set\n",
    "# types = set(types)\n",
    "# print(f\"Types of mean_records: {types}\")\n",
    "print(f\"Updated non-integer records: {updated_non_integer_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean records value for the entire DataFrame df_vis (excluding non-numeric values)\n",
    "df_vis['Records'] = pd.to_numeric(df_vis['Records'], errors='coerce')\n",
    "df_vis = df_vis.dropna()\n",
    "mean_records_all = df_vis['Records'].mean()\n",
    "\n",
    "# Replace 'Records' values in updated_non_integer_records with the mean records for the entire DataFrame\n",
    "for industry in updated_non_integer_records:\n",
    "    # for each entry in the dataframe, if the industry matches the industry in the updated_non_integer_records, replace the value with the mean_records_all\n",
    "    for index, value, industry_name in non_integer_records:\n",
    "        if industry_name == industry:\n",
    "            df_vis.at[index, 'Records'] = mean_records_all\n",
    "#print(df_vis)\n",
    "#Check that there are no more non-integer values in 'Records'\n",
    "non_integer_records = []\n",
    "for index, value, industry in zip(df_vis.index, df_vis['Records'], df_vis['Organization type']):\n",
    "    try:\n",
    "        int_value = int(value)\n",
    "    except ValueError:\n",
    "        non_integer_records.append((index, str(value), industry))\n",
    "\n",
    "if non_integer_records:\n",
    "    print(\"Non-integer values in the 'Records' column:\")\n",
    "    print(\"Index, Value, Industry\")\n",
    "    for index, value, industry in non_integer_records:\n",
    "        print(index, value, industry)\n",
    "else:\n",
    "    print(\"No remaining non-integer values in the 'Records' column!\")\n",
    "\n",
    "#Update all the values in the Records column to be integers\n",
    "df_vis['Records'] = df_vis['Records'].astype(int)\n",
    "\n",
    "# Check the data types of the DataFrame in the 'Records' column\n",
    "print(df_vis.dtypes)\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633714",
   "metadata": {},
   "source": [
    "Checking if the Year column is well formatted (should be a year like 2016, or 2019, not 2016-2019). If not, we'll need to do some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51564148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any value in the year column is null\n",
    "print(\"Null values: \", df_vis['Year'].isnull().values.any())  # there are no null values in the year column\n",
    "\n",
    "#Check if any value in the year column is not well formatted (i.e. not a number)\n",
    "print(\"Non numeric values: \", df_vis['Year'].str.isnumeric().values.any())  # there are values that are not numeric\n",
    "\n",
    "#print out all the values in the year column that are not numeric\n",
    "print(df_vis[~df_vis['Year'].str.isnumeric()])  # as we can see, the three columns that have non-numeric values are in index 94 96 and 144\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40deac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c521a8b",
   "metadata": {},
   "source": [
    "There are 3 values in the Year column that are not well formed â€“ we also will need to fix the Records column for similar formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a650533",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modify df_vis:\n",
    "for each value in the year column that is not numeric:\n",
    "    record the last year listed in the year column (i.e. the last 4 characters)\n",
    "    record the first year listed in the year column (i.e. the first 4 characters)\n",
    "change the year column of this entry to the first 4 characters (the first year)\n",
    "for each year between the first year and the last year:\n",
    "    add a new entry to the dataframe with the same values as the entry that was changed, except for the year column, which will be the year in question\n",
    "'''\n",
    "last_row_index = df_vis.tail(1).index[0]\n",
    "# Create an empty list to store modified rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df_vis.iterrows():\n",
    "    year_value = row['Year']\n",
    "    \n",
    "    # Check if the year is not numeric\n",
    "    if not year_value.isnumeric():\n",
    "        # Extract the first and last year\n",
    "        first_year = int(year_value[:4])\n",
    "        last_year = int(year_value[-4:])\n",
    "        \n",
    "        # Calculate the range of years and ensure it's at least 1\n",
    "        year_range = max(last_year - first_year, 0) + 1\n",
    "\n",
    "        # Change the year column to the first year\n",
    "        df_vis.loc[index, 'Year'] = str(first_year)\n",
    "        \n",
    "        # Calculate the records divided by the number of years in the range\n",
    "        records_divided = row['Records'] / year_range\n",
    "        \n",
    "        # Update the records column with the new value\n",
    "        df_vis.loc[index, 'Records'] = records_divided\n",
    "        \n",
    "        #Create new rows for each year between the first and last year\n",
    "        for year in range(first_year + 1, last_year + 1):\n",
    "            new_row = row.copy()  # Create a copy of the current row\n",
    "            new_row['Year'] = str(year)\n",
    "            new_row['Records'] = records_divided\n",
    "            new_rows.append(new_row) # Append the new row to the list\n",
    "\n",
    "# Concatenate the new rows with the original DataFrame\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "df_vis = pd.concat([df_vis, new_rows_df], ignore_index=True)\n",
    "\n",
    "# Convert Records to int\n",
    "df_vis['Records'] = df_vis['Records'].astype(int)\n",
    "\n",
    "# convert the year column to int\n",
    "df_vis['Year'] = df_vis['Year'].astype(int)\n",
    "\n",
    "\n",
    "print(df_vis.dtypes)\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert spaces, /, \",\", and - to _ in Organization type\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace(' ', '_')\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace('/', '_')\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace('-', '_')\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace(',', '_')\n",
    "\n",
    "#convert spaces, /, \",\", __, and - to _ in Entity\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace(' ', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace('/', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace('-', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace(',', '_')\n",
    "df_vis['Entity'] = df_vis['Entity'].str.replace('__', '_')\n",
    "\n",
    "#print out the unique values in entity\n",
    "print(df_vis['Entity'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are the same entity, such as Yahoo, Sony, Nintendo, British_Airways, Gmail and google, and Adobe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename 'Adobe_Systems_Incorporated and 'Adobe_Inc.' to 'Adobe'\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Adobe_Systems_Incorporated': 'Adobe', 'Adobe_Inc.': 'Adobe'})\n",
    "#Rename \"Yahoo_Japan\" and \"Yahoo!_Voices\" to \"Yahoo\"\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Yahoo_Japan': 'Yahoo', 'Yahoo!_Voices': 'Yahoo'})\n",
    "#Rename \"Sony_Pictures\" and \"Sony_Playstation_Network\" to \"Sony\"\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Sony_Pictures': 'Sony', 'Sony_Playstation_Network': 'Sony'})\n",
    "#Rename \"Nintendo_(Nintendo_Acount)\" and \"Nintendo_(Club_Nintendo) to \"Nintendo\"\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Nintendo_(Nintendo_Acount)': 'Nintendo', 'Nintendo_(Club_Nintendo)': 'Nintendo'})\n",
    "#rename '2018_British_Airways_cyberattack' and 'British_Airways' to 'BritishAirways'\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'2018_British_Airways_cyberattack': 'BritishAirways', 'British_Airways': 'BritishAirways'})\n",
    "#rename 'Gmail' and 'Google_Plus' to 'Google'\n",
    "df_vis['Entity'] = df_vis['Entity'].replace({'Gmail': 'Google', 'Google_Plus': 'Google'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the unique values in entity\n",
    "print(len(df_vis['Entity'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis[\"Organization type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"__\" with \"_\"\n",
    "df_vis['Organization type'] = df_vis['Organization type'].str.replace('__', '_')\n",
    "print(df_vis[\"Organization type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each row in the data frame, \n",
    "for index, row in df_vis.iterrows():\n",
    "#convert the organization type to lower case\n",
    "    df_vis.loc[index, 'Organization type'] = row['Organization type'].lower()\n",
    "# if the organization type is web_ and something else, replace it with the last word. \n",
    "    if row['Organization type'].split('_')[0] == 'web' and len(row['Organization type'].split('_'))>1:\n",
    "        df_vis.loc[index, 'Organization type'] = row['Organization type'].split('_')[-1]\n",
    "#if the organization type is 'health', replace it with 'healthcare'\n",
    "    elif row['Organization type'] == 'health':\n",
    "        df_vis.loc[index, 'Organization type'] = 'healthcare'\n",
    "# if it is telecoms or telecom, replace it with telecommunications\n",
    "    elif row['Organization type'] == 'telecoms' or row['Organization type'] == 'telecom':\n",
    "        df_vis.loc[index, 'Organization type'] = 'telecommunications'\n",
    "#if the organization type is banking, or the first word in the organization type is 'financial', replace it with 'financial'\n",
    "    elif row['Organization type'].split('_')[0] == 'financial' or row['Organization type'] == 'banking':\n",
    "        df_vis.loc[index, 'Organization type'] = 'financial'\n",
    "#if the first word in the organization type is 'social', replace it with 'social'\n",
    "    elif row['Organization type'].split('_')[0] == 'social':\n",
    "        df_vis.loc[index, 'Organization type'] = 'social'\n",
    "#if the first word in the organization type is 'Telephone', replace it with 'telecomunications'\n",
    "    elif row['Organization type'].split('_')[0] == 'Telephone' or row['Organization type'].split('_')[0] == 'telephone':\n",
    "        df_vis.loc[index, 'Organization type'] = 'telecommunications'\n",
    "#if there are two words and the first word in the organization type is 'web', and the second word is not 'tech', replace it with 'the second word\n",
    "    elif (row['Organization type'].split('_')[0] == 'web' and len(row['Organization type'].split('_'))>1 and row['Organization type'].split('_')[-1] != 'tech') or (row['Organization type'] == 'hosting_provider'):\n",
    "        df_vis.loc[index, 'Organization type'] = 'web'\n",
    "#if the first word in the organization type is 'mobile', replace it with 'telecomunications'\n",
    "    elif row['Organization type'].split('_')[0] == 'mobile':\n",
    "        df_vis.loc[index, 'Organization type'] = 'telecommunications'\n",
    "#if the first word is shopping or Consumer, replace it with 'retail'\n",
    "    elif row['Organization type'].split('_')[0] == 'shopping' or row['Organization type'].split('_')[0] == 'Consumer':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the first word is tech, and there is a second word that is retail, replace it with retail\n",
    "    elif row['Organization type'].split('_')[0] == 'tech' and len(row['Organization type'].split('_'))>1 and row['Organization type'].split('_')[-1] == 'retail':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the first word is game, replace it with 'gaming'\n",
    "    elif row['Organization type'].split('_')[0] == 'game':\n",
    "        df_vis.loc[index, 'Organization type'] = 'gaming'\n",
    "#if the organization type is \"phone_accessories\", replace it with \"retail\"\n",
    "    elif row['Organization type'] == 'phone_accessories':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the first word is \"personal\", \"humanitarian, \"special\", or \"local\", replace it with \"demographic\"\n",
    "    elif row['Organization type'].split('_')[0] == 'personal' or row['Organization type'].split('_')[0] == 'humanitarian' or row['Organization type'].split('_')[0] == 'special' or row['Organization type'].split('_')[0] == 'local':\n",
    "        df_vis.loc[index, 'Organization type'] = 'demographic'\n",
    "# if the first word is educational or academic, replace it with \"education\"\n",
    "    elif row['Organization type'].split('_')[0] == 'educational' or row['Organization type'].split('_')[0] == 'academic':\n",
    "        df_vis.loc[index, 'Organization type'] = 'education'\n",
    "#if the first word is messaging, replace it with \"social\"\n",
    "    elif row['Organization type'].split('_')[0] == 'messaging':\n",
    "        df_vis.loc[index, 'Organization type'] = 'social'\n",
    "#if the organization type is online_shopping, replac eit with retail\n",
    "    elif row['Organization type'] == 'online_shopping':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the type is market_analysis, ticket_distribution, or online_marketing, replace it with \"demographic\"\n",
    "    elif row['Organization type'] == 'market_analysis' or row['Organization type'] == 'ticket_distribution' or row['Organization type'] == 'online_marketing':\n",
    "        df_vis.loc[index, 'Organization type'] = 'demographic'\n",
    "#if the first word is \"publisher\", replace it with \"media\"\n",
    "    elif row['Organization type'].split('_')[0] == 'publisher':\n",
    "        df_vis.loc[index, 'Organization type'] = 'media'\n",
    "#if the organization type is QR_code_payment, replace it with \"financial_retail_tech\"\n",
    "    elif row['Organization type'] == 'QR_code_payment':\n",
    "        df_vis.loc[index, 'Organization type'] = 'financial_retail_tech'\n",
    "#if the organization type is 'fashion', replace it with \"retail\"\n",
    "    elif row['Organization type'] == 'fashion' or row['Organization type'] == 'consumer_goods':\n",
    "        df_vis.loc[index, 'Organization type'] = 'retail'\n",
    "#if the organization type is background_check, replace it with \"demographic\"\n",
    "    elif row['Organization type'] == 'background_check' or row['Organization type'] == 'data_broker' or row['Organization type'] == 'question_&_answer':\n",
    "        df_vis.loc[index, 'Organization type'] = 'demographic'\n",
    "#if the organization type is government_database, replace it with \"government\"\n",
    "    elif row['Organization type'] == 'government_database':\n",
    "        df_vis.loc[index, 'Organization type'] = 'government'\n",
    "#replace \"question_&_answer\" with \"demographic\"\n",
    "if df_vis['Organization type'].str.contains('question_&_answer').any():\n",
    "    df_vis['Organization type'] = df_vis['Organization type'].str.replace('question_&_answer', 'demographic')\n",
    "print(df_vis[\"Organization type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa512522e893902e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.878799600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count the unique values in the organization column\n",
    "print(\"Organizations: \", len(df_vis['Organization type'].unique()))  # there are 60 values in the organization column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_rows = []\n",
    "\n",
    "for index, row in df_vis.iterrows():\n",
    "    if row['Organization type'] == \"government_healthcare\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'government'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'healthcare'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "    elif row['Organization type'] == \"military_healthcare\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'military'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'healthcare'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "    elif row['Organization type'] == \"government_military\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'government'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'military'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "    elif row['Organization type'] == \"tech_web\":\n",
    "        new_row = row.copy()\n",
    "        new_row['Organization type'] = 'tech'\n",
    "        new_row['Records'] = row['Records']/2\n",
    "        new_rows.append(new_row)\n",
    "        df_vis.loc[index, 'Organization type'] = 'web'\n",
    "        df_vis.loc[index, 'Records'] = row['Records']/2\n",
    "# Concatenate the new rows with the original DataFrame\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "df_vis = pd.concat([df_vis, new_rows_df], ignore_index=True)\n",
    "print(\"Organizations: \", len(df_vis['Organization type'].unique()))  # there are 60 values in the organization column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any null values in each column\n",
    "if df_vis.isnull().values.any():\n",
    "    print(\"There are null values in the dataframe\")\n",
    "else:\n",
    "    print(\"There are no null values in the dataframe\")\n",
    "\n",
    "print(df_vis.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the lowest and highest year\n",
    "print(\"Lowest Year: \", df_vis['Year'].min())\n",
    "print(\"Highest Year: \", df_vis['Year'].max())\n",
    "\n",
    "# print the lowest and highest records\n",
    "print(\"Lowest Records: \", df_vis['Records'].min())\n",
    "print(\"Highest Records: \", df_vis['Records'].max())\n",
    "print()\n",
    "\n",
    "#print the top 3 organizations with the highest records\n",
    "print(df_vis.groupby('Organization type')['Records'].sum().sort_values(ascending=False).head(3))\n",
    "print()\n",
    "\n",
    "#print the total records for the healthcare industry\n",
    "print(\"Total records lost in the healthcare industry: \")\n",
    "print(df_vis[df_vis['Organization type'] == 'healthcare']['Records'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis)\n",
    "# convert all uppercase letters to lowercase\n",
    "df_vis['Method'] = df_vis['Method'].str.lower()\n",
    "\n",
    "# replace all spaces with \"_\"\n",
    "df_vis['Method'] = df_vis['Method'].str.replace(' ', '_')\n",
    "\n",
    "# replace all slash signs with \"_\"\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('/', '_')\n",
    "\n",
    "# replace all __ and ___ with _\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('___', '_')\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('__', '_')\n",
    "\n",
    "# check the updated dataframe\n",
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_vis.iterrows():\n",
    "    if \"poor_security\" in row[\"Method\"] or \"unsecured_s3_bucket\" in row[\"Method\"] or \"poor_security_hacked\" in row[\"Method\"] or \"misconfiguration\" in row[\"Method\"] or \"improper_setting\" in row[\"Method\"] or \"publicly_accessible\" in row[\"Method\"] or \"unprotected_api\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"poor_security\"\n",
    "    elif \"lost_stolen_media\" in row[\"Method\"] or \"lost_stolen_computer\" in row[\"Method\"]: \n",
    "        df_vis.loc[index, \"Method\"] = \"stolen_media\"\n",
    "    elif \"_hacked\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"hacked\"\n",
    "    elif \"intentionally_lost\" in row[\"Method\"] or \"accidentally_exposed\"  in row[\"Method\"]  or \"accidentally_published\" in row[\"Method\"] or \"accidentally_uploaded\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"accidental_breach\"\n",
    "    elif \"inside_job\" in row[\"Method\"] or \"rogue_contractor\" in row[\"Method\"] or \"social_engineering\" in row[\"Method\"]:\n",
    "        df_vis.loc[index, \"Method\"] = \"inside_job\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of records for each method (excluding \"unknown\")\n",
    "method_records = df_vis[df_vis['Method'] != 'unknown'].groupby('Method')['Records'].sum()\n",
    "\n",
    "# Calculate the total sum of records (excluding \"unknown\")\n",
    "total_records = method_records.sum()\n",
    "\n",
    "# Calculate the percentage of records lost for each method (excluding \"unknown\")\n",
    "method_percentages = (method_records / total_records) * 100\n",
    "\n",
    "# Print out the percentages\n",
    "print(\"Percentage of records lost for each method:\")\n",
    "print(method_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_vis.copy()\n",
    "# rename unknown columns to poor_security\n",
    "df_class['Method'] = df_class['Method'].replace({'unknown': 'poor_security'})\n",
    "print(df_class['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'] = df_vis['Method'].replace({'accidental_breach': 'poor_security', 'inside_job': 'hacked', 'stolen_media': 'poor_security'})\n",
    "print(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of records for the \"unknown\" method\n",
    "unknown_records = df_vis[df_vis['Method'] == 'unknown']['Records'].sum()\n",
    "\n",
    "# Calculate the total sum of records\n",
    "total_records = df_vis['Records'].sum()\n",
    "\n",
    "# Calculate the percentage of \"unknown\" method records lost\n",
    "unknown_percentage = (unknown_records / total_records) * 100\n",
    "\n",
    "# Print the percentage\n",
    "print(\"Percentage of 'unknown' method records lost: \", unknown_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'] = df_vis['Method'].replace('unknown', 'poor_security')\n",
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vis.drop(\"Entity\", axis=1, inplace=True)\n",
    "df_vis[\"Records\"] = df_vis[\"Records\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis.dtypes)\n",
    "print(df_class.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#convert the years to ints\n",
    "df_vis['Year'] = df_vis['Year'].astype(int)\n",
    "# Grouping the data by year\n",
    "grouped_data = df_vis.groupby('Year').size()\n",
    "\n",
    "# Sorting the grouped data by year\n",
    "grouped_data = grouped_data.sort_index()\n",
    "\n",
    "# Convert the years to whole numbers\n",
    "grouped_data.index = grouped_data.index.astype(int)\n",
    "\n",
    "# Creating a bar plot with figsize\n",
    "fig, ax = plt.subplots(figsize=(25, 12))\n",
    "\n",
    "# Use a colormap to create a gradient effect\n",
    "norm = plt.Normalize(0, grouped_data.max())\n",
    "colors = plt.cm.Reds(norm(grouped_data.values))\n",
    "bars = ax.bar(grouped_data.index, grouped_data.values, color=colors)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Entries')\n",
    "ax.set_title('Number of Entries per Year')\n",
    "\n",
    "# Add a colorbar to show the scale\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='Reds'), ax=ax, orientation='vertical')\n",
    "cbar.set_label('Record Count')\n",
    "\n",
    "# Make the background transparent\n",
    "# fig.patch.set_alpha(0.0)\n",
    "# ax.patch.set_alpha(0.0)\n",
    "\n",
    "# Save the plot with a transparent background\n",
    "plt.savefig('graphs/entries.png', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and sum the records for each year\n",
    "data_lost_over_time = df_vis.groupby('Year')['Records'].sum()\n",
    "\n",
    "# Create a line plot for data lost over time\n",
    "data_lost_over_time.plot(kind='line', figsize=(25, 12))\n",
    "plt.title(\"Data Lost Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Data Lost\")\n",
    "#save the plot as \"records_line_plot.png\" in graphs\n",
    "plt.savefig('graphs/records_line_plot.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by industry and sum the records for each industry, then select the top 20 industries\n",
    "top_20_industries = df_vis.groupby('Organization type')['Records'].sum().nlargest(20)\n",
    "\n",
    "# Create a bar plot for the top 10 industries with the highest data loss\n",
    "top_20_industries.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 20 Industries with Highest Data Loss\")\n",
    "plt.xlabel(\"Industry\")\n",
    "plt.ylabel(\"Total Data Lost\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the method and count the occurrences, then select the top 10 causes\n",
    "top_10_causes = df_vis['Method'].value_counts().nlargest(10)\n",
    "\n",
    "# Create a bar plot for the top 10 causes of data breaches\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 10 Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12), yerr=df_vis['Method'].value_counts().std())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_causes = df_class['Method'].value_counts()\n",
    "\n",
    "# Set up a color palette with a unique color for each bar\n",
    "colors = sns.color_palette('viridis', len(top_causes))\n",
    "top_limit = top_causes.max() + 20\n",
    "plt.ylim(bottom=0, top = top_limit)\n",
    "# Create a bar plot for the causes of data breaches\n",
    "top_causes.plot(kind='bar', figsize=(12, 6), color=colors, yerr=df_class['Method'].value_counts().std())\n",
    "plt.title(\"Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add labels to each bar containing the count of entries for each cause. offset the labels to the right by 0.1\n",
    "for index, value in enumerate(top_causes):\n",
    "    plt.text(index + 0.145, value + 1, str(value), ha='center', va='bottom' , rotation=45, fontweight='bold', color='black', fontsize=12)\n",
    "\n",
    "\n",
    "#save the plot as \"breachcauses.png\" in the directory called graphs\n",
    "plt.savefig('graphs/breachcauses.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we calculate the error bars, theres an upper and lower bound, the upper bound is the mean + the standard deviation, the lower bound is the mean - the standard deviation.\n",
    "So, drawing a line through the lower bound, and it doesnt touch the other line, that means that the difference is statistically significant.\n",
    "\n",
    "We may want to exclude the data that is statistically significant, because it may be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(20).index\n",
    "\n",
    "# Filtering the DataFrame to include only the top 20 industries\n",
    "records_by_year_and_industry_top20 = records_by_year_and_industry[top_20_industries]\n",
    "\n",
    "# Creating a stacked bar plot for records by year and industry (top 20)\n",
    "records_by_year_and_industry_top20.plot(kind='bar', stacked=True, figsize=(25, 12))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title(\"Stacked Bar Plot of Records by Year and Top 20 Industries\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Records\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='Industry Type', bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\n",
    "\n",
    "#save the plot as \"industryrecords.png\" in the directory called graphs\n",
    "plt.savefig('graphs/industryrecords.png', bbox_inches='tight')\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory to save the plots\n",
    "output_directory = 'graphs'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "organization_types = df_class['Organization type'].unique()\n",
    "# Iterate over the organization types and create a pie chart for each\n",
    "for org_type in organization_types:\n",
    "    # Filter the data for the current organization type\n",
    "    org_data = df_class[df_class['Organization type'] == org_type]\n",
    "    \n",
    "    # Get the unique methods for the current organization type\n",
    "    methods = org_data['Method'].unique()\n",
    "    \n",
    "    # Get the counts for each method\n",
    "    method_counts = org_data['Method'].value_counts()\n",
    "    \n",
    "    # Generate a list of colors for the pie chart\n",
    "    colors = plt.cm.Set3(range(len(methods)))\n",
    "    \n",
    "    # Create the pie chart\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.pie(method_counts, labels=methods, colors=colors, autopct='%1.1f%%')\n",
    "    plt.title(org_type)\n",
    "    \n",
    "    # Save the plot as a .png file in the specified directory\n",
    "    output_filename = os.path.join(output_directory, f'{org_type}_breach_causes.png')\n",
    "    plt.savefig(output_filename)\n",
    "    plt.show()\n",
    "    # Close the current plot to free up resources for the next iteration\n",
    "    plt.close()\n",
    "\n",
    "print(\"Plots saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(13).index\n",
    "\n",
    "# Filtering the DataFrame to include all industries and filling NaN values with 0\n",
    "records_by_year_and_industry_all = records_by_year_and_industry.fillna(0)\n",
    "\n",
    "# Create a consolidated image for all pie charts\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Create subplots based on the number of years\n",
    "num_years = len(records_by_year_and_industry_all.index)\n",
    "num_cols = 4 \n",
    "num_rows = (num_years + num_cols - 1) // num_cols\n",
    "\n",
    "\n",
    "for i, year in enumerate(records_by_year_and_industry_all.index, start=1):\n",
    "    data_for_year = records_by_year_and_industry_all.loc[year]\n",
    "\n",
    "    # Calculate the percentage of records lost in the healthcare industry and the rest\n",
    "    #the total number of records lost that year where \"Organization type == healthcare\"\n",
    "    healthcare_sum = data_for_year.groupby('Organization type').sum()['healthcare']\n",
    "    healthcare_percentage = healthcare_sum / data_for_year.sum()\n",
    "    other_percentage = 1 - healthcare_percentage\n",
    "\n",
    "    # Create subplots\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.pie([healthcare_percentage, other_percentage], labels=['healthcare', 'other'],\n",
    "            autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "            startangle=90)\n",
    "    plt.title(f\"{year}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(13).index\n",
    "\n",
    "# Filtering the DataFrame to include all industries and filling NaN values with 0\n",
    "records_by_year_and_industry_all = records_by_year_and_industry.fillna(0)\n",
    "wp = { 'linewidth' : 1, 'edgecolor' : \"black\" }\n",
    "# Creating explode data\n",
    "explode = (0.1, 0.0)\n",
    "# Create subplots for each year\n",
    "for i, year in enumerate(records_by_year_and_industry_all.index, start=1):\n",
    "    # Create a separate figure for each year\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    data_for_year = records_by_year_and_industry_all.loc[year]\n",
    "\n",
    "    # Calculate the percentage of records lost in the healthcare industry and the rest\n",
    "    # the total number of records lost that year where \"Organization type == healthcare\"\n",
    "    healthcare_sum = data_for_year.groupby('Organization type').sum().get('healthcare', 0)\n",
    "    healthcare_percentage = healthcare_sum / data_for_year.sum()\n",
    "    if healthcare_percentage > 0.02:\n",
    "        \n",
    "        other_percentage = 1 - healthcare_percentage\n",
    "\n",
    "        # Create subplots\n",
    "        plt.pie([healthcare_percentage, other_percentage], labels=['Healthcare', 'Other'],\n",
    "                autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '', wedgeprops=wp, explode = explode,\n",
    "                colors = ['dodgerblue', 'gold'],\n",
    "                startangle=90)\n",
    "        plt.title(f\"{year}\")\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(healthcare_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis)\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()  #perform label encoding on the organization_type, the copy of df_vis\n",
    "\n",
    "# implement label encoding on the Organization type column\n",
    "le.fit(df_vis['Organization type'])   # fit the data we want to train the encoder on\n",
    "df_vis['Organization type'] = le.transform(df_vis['Organization type'])\n",
    "# observe how the column 'Organization Type' has changed\n",
    "df_vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.dtypes   # as we can see, the Organization type changed from Object --> integer datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the label frequency, to gain an understanding of outliers and inlier values\n",
    "df_vis['Organization type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['Method'].value_counts()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which columns have null/unknoown values\n",
    "null_mask = df_vis.isnull().any(axis=1)\n",
    "null_rows=df_vis[null_mask]\n",
    "\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.isnull().all()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_vis['Method'].unique()))  \n",
    "print(len(list(df_vis['Method'].value_counts())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le.fit(df_vis['Method'])  # train the label encoder on the column data we want to train\n",
    "df_vis['Method'] = le.transform(df_vis['Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READY TO TRAIN â€“ Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Organization type']\n",
    "label = 'Method'\n",
    "\n",
    "X = df_vis[features]\n",
    "y = df_vis[label]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_standardized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.to_numpy(), dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Define the neural network model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_splits = 15\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the performance metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tensor)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_tensor[train_index], X_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_tensor[train_index], y_tensor[val_index]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for start in range(0, len(X_train_fold), batch_size):\n",
    "            X_batch = X_train_fold[start:start+batch_size]\n",
    "            y_batch = y_train_fold[start:start+batch_size]\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model(X_val_fold)\n",
    "        mse_val = loss_fn(y_pred_val, y_val_fold)\n",
    "        fold_metrics.append(mse_val.item())\n",
    "        y_pred_binary = (y_pred_val >= 0.5).float()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (y_pred_binary == y_val_fold).sum().item()\n",
    "        total = y_val_fold.size(0)\n",
    "        accuracy = correct / total * 100.0\n",
    "\n",
    "        print(f\"Fold {fold + 1} - Validation MSE: {mse_val:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold 1/15\n",
    "Fold 1 - Validation MSE: 0.6542 - Accuracy: 70.83%\n",
    "\n",
    "Fold 2/15\n",
    "Fold 2 - Validation MSE: 0.6355 - Accuracy: 54.17%\n",
    "\n",
    "Fold 3/15\n",
    "Fold 3 - Validation MSE: 0.7007 - Accuracy: 50.00%\n",
    "\n",
    "Fold 4/15\n",
    "Fold 4 - Validation MSE: 0.7450 - Accuracy: 45.83%\n",
    "\n",
    "Fold 5/15\n",
    "Fold 5 - Validation MSE: 0.6813 - Accuracy: 41.67%\n",
    "\n",
    "Fold 6/15\n",
    "Fold 6 - Validation MSE: 0.6768 - Accuracy: 62.50%\n",
    "\n",
    "Fold 7/15\n",
    "Fold 7 - Validation MSE: 0.6519 - Accuracy: 69.57%\n",
    "\n",
    "Fold 8/15\n",
    "Fold 8 - Validation MSE: 0.6211 - Accuracy: 73.91%\n",
    "\n",
    "Fold 9/15\n",
    "Fold 9 - Validation MSE: 0.6427 - Accuracy: 52.17%\n",
    "\n",
    "Fold 10/15\n",
    "Fold 10 - Validation MSE: 0.6925 - Accuracy: 60.87%\n",
    "\n",
    "Fold 11/15\n",
    "Fold 11 - Validation MSE: 0.6186 - Accuracy: 60.87%\n",
    "\n",
    "Fold 12/15\n",
    "Fold 12 - Validation MSE: 0.5808 - Accuracy: 69.57%\n",
    "\n",
    "Fold 13/15\n",
    "Fold 13 - Validation MSE: 0.6072 - Accuracy: 69.57%\n",
    "\n",
    "Fold 14/15\n",
    "Fold 14 - Validation MSE: 0.7312 - Accuracy: 60.87%\n",
    "\n",
    "Fold 15/15\n",
    "Fold 15 - Validation MSE: 0.5168 - Accuracy: 82.61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization on multiclass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by organization type and method, and sum the records for each combination\n",
    "grouped_data = df_class.groupby(['Organization type', 'Method'])['Records'].sum().unstack()\n",
    "\n",
    "# Create a stacked bar plot\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Records by Organization Type and Method')\n",
    "plt.xlabel('Organization Type')\n",
    "plt.ylabel('Records')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network â€“ Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class.drop(\"Entity\", axis=1, inplace=True)\n",
    "df_class[\"Records\"] = df_class[\"Records\"].astype(int)\n",
    "# Group the data by the method and count the occurrences, then select the top 10 causes\n",
    "top_10_causes = df_class['Method'].value_counts().nlargest(10)\n",
    "\n",
    "# Create a bar plot for the top 10 causes of data breaches\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 10 Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12), yerr=df_class['Method'].value_counts().std())\n",
    "plt.show()\n",
    "\n",
    "print(df_class['Method'].unique())\n",
    "print(df_class['Method'].value_counts())\n",
    "\n",
    "le = LabelEncoder()  #perform label encoding on the organization_type, the copy of df_vis\n",
    "\n",
    "# implement label encoding on the Organization type column\n",
    "le.fit(df_class['Organization type'])   # fit the data we want to train the encoder on\n",
    "df_class['Organization type'] = le.transform(df_class['Organization type'])\n",
    "le.fit(df_class['Method'])  # train the label encoder on the column data we want to train\n",
    "df_class['Method'] = le.transform(df_class['Method'])\n",
    "print(df_class.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 5)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_splits = 15\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the performance metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tensor)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_tensor[train_index], X_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_tensor[train_index], y_tensor[val_index]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for start in range(0, len(X_train_fold), batch_size):\n",
    "            X_batch = X_train_fold[start:start+batch_size]\n",
    "            y_batch = y_train_fold[start:start+batch_size]\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            \n",
    "            # Ensure that y_batch contains class indices\n",
    "            y_batch = y_batch.long().squeeze(dim=1)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model(X_val_fold)\n",
    "        \n",
    "        # Ensure that y_val_fold contains class indices\n",
    "        y_val_fold = y_val_fold.long().squeeze(dim=1)\n",
    "\n",
    "        loss_val = loss_fn(y_pred_val, y_val_fold)\n",
    "        fold_metrics.append(loss_val.item())\n",
    "\n",
    "        # Convert logits to predicted class\n",
    "        y_pred_classes = torch.argmax(y_pred_val, dim=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (y_pred_classes == y_val_fold).sum().item()\n",
    "        total = y_val_fold.size(0)\n",
    "        accuracy = correct / total * 100.0\n",
    "\n",
    "        print(f\"Fold {fold + 1} - Validation Loss: {loss_val:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold 1/15\n",
    "Fold 1 - Validation Loss: 0.6366 - Accuracy: 70.83%\n",
    "\n",
    "Fold 2/15\n",
    "Fold 2 - Validation Loss: 0.6561 - Accuracy: 54.17%\n",
    "\n",
    "Fold 3/15\n",
    "Fold 3 - Validation Loss: 0.7103 - Accuracy: 50.00%\n",
    "\n",
    "Fold 4/15\n",
    "Fold 4 - Validation Loss: 0.7385 - Accuracy: 45.83%\n",
    "\n",
    "Fold 5/15\n",
    "Fold 5 - Validation Loss: 0.7130 - Accuracy: 41.67%\n",
    "\n",
    "Fold 6/15\n",
    "Fold 6 - Validation Loss: 0.6728 - Accuracy: 62.50%\n",
    "\n",
    "Fold 7/15\n",
    "Fold 7 - Validation Loss: 0.6652 - Accuracy: 69.57%\n",
    "\n",
    "Fold 8/15\n",
    "Fold 8 - Validation Loss: 0.6181 - Accuracy: 73.91%\n",
    "\n",
    "Fold 9/15\n",
    "Fold 9 - Validation Loss: 0.6851 - Accuracy: 52.17%\n",
    "\n",
    "Fold 10/15\n",
    "Fold 10 - Validation Loss: 0.6841 - Accuracy: 60.87%\n",
    "\n",
    "Fold 11/15\n",
    "Fold 11 - Validation Loss: 0.6212 - Accuracy: 60.87%\n",
    "\n",
    "Fold 12/15\n",
    "Fold 12 - Validation Loss: 0.5791 - Accuracy: 73.91%\n",
    "\n",
    "Fold 13/15\n",
    "Fold 13 - Validation Loss: 0.6183 - Accuracy: 69.57%\n",
    "\n",
    "Fold 14/15\n",
    "Fold 14 - Validation Loss: 0.7136 - Accuracy: 60.87%\n",
    "\n",
    "Fold 15/15\n",
    "Fold 15 - Validation Loss: 0.5137 - Accuracy: 82.61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi class classification with Organization type and Records as the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "features = ['Organization type', 'Records']\n",
    "label = 'Method'\n",
    "\n",
    "X = df_class[features]\n",
    "y = df_class[label]\n",
    "\n",
    "# Use LabelEncoder for 'Organization type' to convert it into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "X['Organization type'] = label_encoder.fit_transform(X['Organization type'])\n",
    "\n",
    "# One-hot encode 'Organization type'\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "org_type_encoded = onehot_encoder.fit_transform(X['Organization type'].values.reshape(-1, 1))\n",
    "org_type_encoded = pd.DataFrame(org_type_encoded, columns=[f'org_type_{i}' for i in range(org_type_encoded.shape[1])])\n",
    "\n",
    "# Concatenate the one-hot encoded 'Organization type' with the 'Records' column\n",
    "X = pd.concat([org_type_encoded, X['Records']], axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_standardized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.to_numpy(), dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Define the neural network model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_tensor.shape[1], 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_splits = 15\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the performance metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tensor)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_tensor[train_index], X_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_tensor[train_index], y_tensor[val_index]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for start in range(0, len(X_train_fold), batch_size):\n",
    "            X_batch = X_train_fold[start:start+batch_size]\n",
    "            y_batch = y_train_fold[start:start+batch_size].squeeze().long()\n",
    "\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model(X_val_fold).squeeze()\n",
    "        mse_val = loss_fn(y_pred_val, y_val_fold.squeeze().long())\n",
    "        fold_metrics.append(mse_val.item())\n",
    "        \n",
    "        y_pred_classes = torch.argmax(y_pred_val, dim=1)[:len(y_val_fold)]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = (y_pred_classes == y_val_fold.squeeze()).sum().item()\n",
    "        total = len(y_val_fold)\n",
    "        accuracy = correct / total * 100.0\n",
    "        print(f\"Fold {fold + 1} - Validation MSE: {mse_val:.4f} - Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold 1/15\n",
    "Fold 1 - Validation MSE: 1.3994 - Accuracy: 66.67%\n",
    "\n",
    "Fold 2/15\n",
    "Fold 2 - Validation MSE: 1.5664 - Accuracy: 45.83%\n",
    "\n",
    "Fold 3/15\n",
    "Fold 3 - Validation MSE: 1.4994 - Accuracy: 54.17%\n",
    "\n",
    "Fold 4/15\n",
    "Fold 4 - Validation MSE: 1.5260 - Accuracy: 50.00%\n",
    "\n",
    "Fold 5/15\n",
    "Fold 5 - Validation MSE: 1.6066 - Accuracy: 41.67%\n",
    "\n",
    "Fold 6/15\n",
    "Fold 6 - Validation MSE: 1.3667 - Accuracy: 70.83%\n",
    "\n",
    "Fold 7/15\n",
    "Fold 7 - Validation MSE: 1.4249 - Accuracy: 65.22%\n",
    "\n",
    "Fold 8/15\n",
    "Fold 8 - Validation MSE: 1.3730 - Accuracy: 69.57%\n",
    "\n",
    "Fold 9/15\n",
    "Fold 9 - Validation MSE: 1.5247 - Accuracy: 47.83%\n",
    "\n",
    "Fold 10/15\n",
    "Fold 10 - Validation MSE: 1.5220 - Accuracy: 52.17%\n",
    "\n",
    "Fold 11/15\n",
    "Fold 11 - Validation MSE: 1.4663 - Accuracy: 52.17%\n",
    "\n",
    "Fold 12/15\n",
    "Fold 12 - Validation MSE: 1.3240 - Accuracy: 69.57%\n",
    "\n",
    "Fold 13/15\n",
    "Fold 13 - Validation MSE: 1.4824 - Accuracy: 56.52%\n",
    "\n",
    "Fold 14/15\n",
    "Fold 14 - Validation MSE: 1.4123 - Accuracy: 60.87%\n",
    "\n",
    "Fold 15/15\n",
    "Fold 15 - Validation MSE: 1.2891 - Accuracy: 78.26%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification: Optimize number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare the data\n",
    "features = ['Organization type']\n",
    "label = 'Method'\n",
    "\n",
    "X = df_vis[features]\n",
    "y = df_vis[label]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (20,), (30,), (40,), (50,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='roc_auc', cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and its AUC score on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df_class.drop('Method', axis=1)\n",
    "y = df_class['Method']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Define the neural network model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train_tensor.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 5),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor)\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "\n",
    "    train_accuracy = (y_pred_train.argmax(dim=1) == y_train_tensor).float().mean().item()\n",
    "    test_accuracy = (y_pred_test.argmax(dim=1) == y_test_tensor).float().mean().item()\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
